"""è®ºæ–‡ç›¸å…³å·¥å…·
åŒ…å«è®ºæ–‡æœç´¢ã€æ£€ç´¢ç­‰åŠŸèƒ½
"""

from langchain_community.retrievers import ArxivRetriever
from langchain.tools import tool
from pydantic import BaseModel, Field
from loguru import logger
import requests
from typing import Optional, Literal, List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
from difflib import SequenceMatcher


@tool
def search_papers_ArXiv(query: str, max_results: int = 5) -> str:
    """æœç´¢ ArXiv å­¦æœ¯è®ºæ–‡ï¼ŒArXiv ä¸»è¦åŒ…å«è®¡ç®—æœºã€ç‰©ç†ã€æ•°å­¦ç­‰é¢†åŸŸçš„é¢„å°æœ¬è®ºæ–‡ã€‚

    ä½¿ç”¨æ­¤å·¥å…·åœ¨ ArXiv ä¸Šæœç´¢å­¦æœ¯è®ºæ–‡ã€‚è¿”å›çš„ç»“æœåŒ…æ‹¬è®ºæ–‡æ ‡é¢˜ã€ä½œè€…ã€å‘è¡¨æ—¥æœŸã€ArXiv IDã€
    è®ºæ–‡é“¾æ¥ã€PDF ä¸‹è½½é“¾æ¥å’Œæ‘˜è¦ã€‚

    Args:
        query: æœç´¢å…³é”®è¯æˆ–çŸ­è¯­ï¼Œä¾‹å¦‚ "machine learning" æˆ– "quantum computing"
        max_results: è¿”å›çš„æœ€å¤§è®ºæ–‡æ•°é‡ï¼Œé»˜è®¤ä¸º 5 ç¯‡ï¼ŒèŒƒå›´ 1-10
    """
    logger.info(
        f"[search_papers] å¼€å§‹ä½¿ç”¨ ArxivRetriever æœç´¢è®ºæ–‡ï¼ŒæŸ¥è¯¢ï¼š{query}ï¼Œæœ€å¤§ç»“æœæ•°ï¼š{max_results}"
    )

    try:
        # åˆ›å»º ArxivRetriever å®ä¾‹
        retriever = ArxivRetriever(
            load_max_docs=max_results,
            get_full_documents=False,  # ä¸ä¸‹è½½å®Œæ•´ PDFï¼Œåªè·å–å…ƒæ•°æ®ï¼ˆæé€Ÿï¼‰
        )

        # è°ƒç”¨ retriever è·å–æ–‡æ¡£
        documents = retriever.invoke(query)

        # æ ¼å¼åŒ–è¾“å‡ºç»“æœ
        if not documents:
            output = f"æœªæ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„è®ºæ–‡ã€‚"
            logger.warning("[search_papers] æœªæ‰¾åˆ°ç»“æœ")
        else:
            output = f"æ‰¾åˆ° {len(documents)} ç¯‡ç›¸å…³è®ºæ–‡ï¼š\n\n"

            for idx, doc in enumerate(documents, 1):
                # æå– metadata ä¸­çš„ä¿¡æ¯
                metadata = doc.metadata
                title = metadata.get("Title", "æœªçŸ¥æ ‡é¢˜")
                authors = metadata.get("Authors", "æœªçŸ¥ä½œè€…")
                published = metadata.get("Published", "æœªçŸ¥æ—¥æœŸ")
                entry_id = metadata.get("Entry ID", "")

                # ä» entry_id æå– ArXiv IDï¼ˆæ ¼å¼ï¼šhttp://arxiv.org/abs/2301.12345v1ï¼‰
                arxiv_id = "æœªçŸ¥"
                if entry_id:
                    # ç§»é™¤ç‰ˆæœ¬å·ï¼Œæå–çº¯ ID
                    if "arxiv.org/abs/" in entry_id:
                        arxiv_id = entry_id.split("arxiv.org/abs/")[-1]
                    else:
                        arxiv_id = entry_id.split("/")[-1]
                    # ç§»é™¤ç‰ˆæœ¬å·ï¼ˆå¦‚ v1, v2ï¼‰
                    if "v" in arxiv_id:
                        arxiv_id = arxiv_id.split("v")[0]

                # è·å–æ‘˜è¦ï¼ˆä» document å†…å®¹ä¸­æˆªå–ï¼‰
                summary = (
                    doc.page_content[:300] + "..."
                    if len(doc.page_content) > 300
                    else doc.page_content
                )

                output += f"{idx}. **{title}**\n"
                output += f"   - ä½œè€…: {authors}\n"
                output += f"   - å‘å¸ƒæ—¥æœŸ: {published}\n"
                output += f"   - ArXiv ID: {arxiv_id}\n"

                # ç”Ÿæˆè®ºæ–‡é“¾æ¥
                if arxiv_id != "æœªçŸ¥":
                    paper_url = f"https://arxiv.org/abs/{arxiv_id}"
                    pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                    output += f"   - è®ºæ–‡é“¾æ¥: {paper_url}\n"
                    output += f"   - PDF ä¸‹è½½: {pdf_url}\n"

                output += f"   - æ‘˜è¦: {summary}\n\n"

            logger.info(f"[search_papers] æˆåŠŸè¿”å› {len(documents)} ç¯‡è®ºæ–‡")

        return output

    except Exception as e:
        error_msg = f"æœç´¢è®ºæ–‡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        logger.error(f"[search_papers] {error_msg}")
        logger.exception(e)  # è®°å½•å®Œæ•´çš„å¼‚å¸¸å †æ ˆ
        return error_msg


class OpenAlexSearchInput(BaseModel):
    """OpenAlex è®ºæ–‡æœç´¢çš„è¾“å…¥å‚æ•°"""

    query: str = Field(
        description="""æœç´¢å…³é”®è¯æˆ–çŸ­è¯­ï¼Œä¼šåœ¨è®ºæ–‡æ ‡é¢˜ã€æ‘˜è¦å’Œå…¨æ–‡ä¸­æœç´¢ã€‚
        
ç¤ºä¾‹ï¼š
- 'machine learning'ï¼ˆæœºå™¨å­¦ä¹ ï¼‰
- 'CRISPR gene editing'ï¼ˆåŸºå› ç¼–è¾‘ï¼‰
- 'climate change impact'ï¼ˆæ°”å€™å˜åŒ–å½±å“ï¼‰

æç¤ºï¼šä½¿ç”¨è‹±æ–‡å…³é”®è¯æ•ˆæœæœ€ä½³ï¼Œå¯ä»¥ä½¿ç”¨å¤šä¸ªè¯ç»„åˆ"""
    )

    max_results: int = Field(
        default=10,
        description="""è¿”å›çš„æœ€å¤§è®ºæ–‡æ•°é‡ã€‚
        
é»˜è®¤ï¼š10 ç¯‡
å»ºè®®èŒƒå›´ï¼š5-50 ç¯‡
æœ€å¤§ï¼š200 ç¯‡

ç”¨æˆ·è¯´æ³•æ˜ å°„ï¼š
- 'å‡ ç¯‡' / 'ä¸€äº›' â†’ 5-10
- 'å¾ˆå¤š' / 'å°½å¯èƒ½å¤š' â†’ 50-100
- æœªæåŠ â†’ ä½¿ç”¨é»˜è®¤ 10""",
        ge=1,
        le=200,
    )

    sort_by: Literal["relevance", "publication_date", "cited_by_count"] = Field(
        default="relevance",
        description="""ç»“æœæ’åºæ–¹å¼ã€‚
        
é€‰é¡¹è¯´æ˜ï¼š
- 'relevance'ï¼ˆç›¸å…³æ€§ï¼‰ï¼šæŒ‰ä¸æœç´¢è¯çš„åŒ¹é…åº¦æ’åºã€é»˜è®¤ã€‘
- 'publication_date'ï¼ˆå‘è¡¨æ—¥æœŸï¼‰ï¼šæŒ‰å‘è¡¨æ—¶é—´ä»æ–°åˆ°æ—§æ’åº
- 'cited_by_count'ï¼ˆå¼•ç”¨æ¬¡æ•°ï¼‰ï¼šæŒ‰è¢«å¼•ç”¨æ¬¡æ•°ä»é«˜åˆ°ä½æ’åº

ç”¨æˆ·è¯´æ³•æ˜ å°„ï¼š
- 'æœ€ç›¸å…³çš„' / 'åŒ¹é…åº¦é«˜çš„' â†’ relevance
- 'æœ€æ–°çš„' / 'æœ€è¿‘çš„' / 'æœ€è¿‘å‘è¡¨çš„' â†’ publication_date  
- 'æœ€æœ‰å½±å“åŠ›' / 'è¢«å¼•ç”¨æœ€å¤š' / 'é«˜å¼•ç”¨' â†’ cited_by_count
- æœªæåŠ â†’ ä½¿ç”¨é»˜è®¤ relevance""",
    )

    publication_year: Optional[str] = Field(
        default=None,
        description="""å‘è¡¨å¹´ä»½ç­›é€‰ï¼Œæ”¯æŒå¤šç§æ ¼å¼ã€‚
        
æ ¼å¼è¯´æ˜ï¼š
- '2023'ï¼šç²¾ç¡®åŒ¹é… 2023 å¹´
- '>2020'ï¼š2021 å¹´åŠä»¥åï¼ˆ2021, 2022, 2023...ï¼‰
- '<2020'ï¼š2019 å¹´åŠä¹‹å‰
- '2020-2023'ï¼š2020 åˆ° 2023 å¹´ä¹‹é—´ï¼ˆåŒ…å«è¾¹ç•Œï¼‰

ç”¨æˆ·è¯´æ³•æ˜ å°„ï¼š
- 'æœ€è¿‘' / 'è¿‘æœŸ' / 'æœ€è¿‘å‡ å¹´' â†’ '>2021' æˆ– '>2022'
- '2023å¹´' / 'å»å¹´' â†’ '2023'
- '2020å¹´åˆ°2023å¹´' / 'è¿‘ä¸‰å¹´' â†’ '2020-2023'
- '2020å¹´ä¹‹å' â†’ '>2020'
- æœªæåŠ â†’ Noneï¼ˆä¸é™åˆ¶å¹´ä»½ï¼‰

æ³¨æ„ï¼šå½“å‰å¹´ä»½æ˜¯ 2024 å¹´ï¼Œè¯·æ®æ­¤è®¡ç®—ç›¸å¯¹æ—¶é—´""",
    )

    open_access_only: bool = Field(
        default=False,
        description="""æ˜¯å¦ä»…è¿”å›å¼€æ”¾è·å–ï¼ˆOpen Access, OAï¼‰è®ºæ–‡ã€‚
        
- Trueï¼šåªè¿”å›å¯ä»¥å…è´¹ä¸‹è½½å…¨æ–‡çš„è®ºæ–‡
- Falseï¼šè¿”å›æ‰€æœ‰è®ºæ–‡ï¼ˆåŒ…æ‹¬éœ€è¦è®¢é˜…çš„ï¼‰ã€é»˜è®¤ã€‘

ç”¨æˆ·è¯´æ³•æ˜ å°„ï¼š
- 'å…è´¹' / 'å…è´¹ä¸‹è½½' / 'å¼€æ”¾è·å–' / 'OA' / 'èƒ½ä¸‹è½½çš„' â†’ True
- 'ä¸é™' / æœªæåŠ â†’ False""",
    )

    cited_by_count_min: Optional[int] = Field(
        default=None,
        description="""æœ€å°å¼•ç”¨æ¬¡æ•°ç­›é€‰ï¼Œåªè¿”å›è¢«å¼•ç”¨æ¬¡æ•°è¾¾åˆ°æ­¤å€¼çš„è®ºæ–‡ã€‚
        
ç”¨äºç­›é€‰é«˜å½±å“åŠ›çš„è®ºæ–‡ã€‚

ç”¨æˆ·è¯´æ³•æ˜ å°„ï¼š
- 'é«˜å¼•ç”¨' / 'æœ‰å½±å“åŠ›çš„' â†’ 50 æˆ– 100
- 'è¢«å¼•ç”¨å¾ˆå¤š' â†’ 100
- 'è‡³å°‘è¢«å¼•ç”¨Xæ¬¡' â†’ X
- 'å¼•ç”¨æ¬¡æ•°è¶…è¿‡X' â†’ X
- æœªæåŠ â†’ Noneï¼ˆä¸é™åˆ¶å¼•ç”¨æ¬¡æ•°ï¼‰

å‚è€ƒå€¼ï¼š
- 10+ï¼šæœ‰ä¸€å®šå½±å“åŠ›
- 50+ï¼šè¾ƒé«˜å½±å“åŠ›  
- 100+ï¼šé«˜å½±å“åŠ›
- 500+ï¼šéå¸¸é«˜å½±å“åŠ›""",
    )


@tool(args_schema=OpenAlexSearchInput)
def search_papers_openalex(
    query: str,
    max_results: int = 10,
    sort_by: Literal["relevance", "publication_date", "cited_by_count"] = "relevance",
    publication_year: Optional[str] = None,
    open_access_only: bool = False,
    cited_by_count_min: Optional[int] = None,
) -> str:
    """æœç´¢ OpenAlex å­¦æœ¯è®ºæ–‡æ•°æ®åº“ - 240M+ ç¯‡å­¦æœ¯æ–‡çŒ®ï¼Œè¦†ç›–æ‰€æœ‰å­¦ç§‘é¢†åŸŸã€‚

    OpenAlex æ˜¯å¼€æ”¾çš„å­¦æœ¯æ•°æ®åº“ï¼ŒåŒ…å«æœŸåˆŠæ–‡ç« ã€ä¼šè®®è®ºæ–‡ã€é¢„å°æœ¬ç­‰ã€‚æä¾›å¼•ç”¨æ•°æ®ã€
    å¼€æ”¾è·å–çŠ¶æ€ã€ä¸»é¢˜åˆ†ç±»ç­‰ä¸°å¯Œä¿¡æ¯ã€‚é€‚åˆå„å­¦ç§‘é¢†åŸŸçš„è®ºæ–‡æœç´¢å’Œå­¦æœ¯ç ”ç©¶ã€‚

    Args:
        query: æœç´¢å…³é”®è¯ï¼Œåœ¨æ ‡é¢˜ã€æ‘˜è¦ã€å…¨æ–‡ä¸­æœç´¢
        max_results: è¿”å›è®ºæ–‡æ•°é‡ï¼Œé»˜è®¤ 10 ç¯‡ï¼Œæœ€å¤§ 200 ç¯‡
        sort_by: æ’åºæ–¹å¼ï¼Œé»˜è®¤ç›¸å…³æ€§
        publication_year: å¹´ä»½ç­›é€‰ï¼Œæ”¯æŒå•å¹´ã€èŒƒå›´ã€æ¯”è¾ƒè¿ç®—
        open_access_only: æ˜¯å¦ä»…è¿”å›å¯å…è´¹è®¿é—®çš„å¼€æ”¾è·å–è®ºæ–‡
        cited_by_count_min: æœ€å°å¼•ç”¨æ¬¡æ•°ï¼Œç”¨äºç­›é€‰é«˜å½±å“åŠ›è®ºæ–‡
    """
    logger.info(
        f"[search_papers_openalex] æœç´¢è®ºæ–‡ - æŸ¥è¯¢: {query}, æ•°é‡: {max_results}, æ’åº: {sort_by}"
    )
    # æ‰“å°æœç´¢å‚æ•°
    logger.info(f"[search_papers_openalex] æœç´¢å‚æ•°: {locals()}")

    try:
        # æ˜ å°„æ’åºå‚æ•°åˆ° OpenAlex API æ ¼å¼
        sort_mapping = {
            "relevance": "relevance_score:desc",
            "publication_date": "publication_date:desc",
            "cited_by_count": "cited_by_count:desc",
        }
        api_sort = sort_mapping.get(sort_by, "relevance_score:desc")

        # æ„å»ºè¿‡æ»¤æ¡ä»¶åˆ—è¡¨ï¼ˆOpenAlex ä½¿ç”¨é€—å·åˆ†éš”çš„ filter å‚æ•°ï¼‰
        filters = []

        # å¹´ä»½ç­›é€‰
        if publication_year:
            filters.append(f"publication_year:{publication_year}")

        # å¼€æ”¾è·å–ç­›é€‰
        if open_access_only:
            filters.append("is_oa:true")

        # æœ€å°å¼•ç”¨æ¬¡æ•°ç­›é€‰
        if cited_by_count_min is not None:
            filters.append(f"cited_by_count:>{cited_by_count_min - 1}")

        # æ„å»º API è¯·æ±‚å‚æ•°
        base_url = "https://api.openalex.org/works"
        params = {
            "search": query,
            "per_page": max_results,  # OpenAlex æœ€ä½³å®è·µï¼šä½¿ç”¨æ›´å¤§çš„ per_page å€¼
            "sort": api_sort,
            "mailto": "347699233@qq.com",  # Polite pool: 10 req/sec (vs 1 req/sec)
        }

        # æ·»åŠ è¿‡æ»¤æ¡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
        if filters:
            params["filter"] = ",".join(filters)

        logger.debug(f"[search_papers_openalex] API å‚æ•°: {params}")

        # å‘é€ HTTP è¯·æ±‚ï¼ˆè¶…æ—¶ 30 ç§’ï¼ŒæŒ‰æ–‡æ¡£å»ºè®®ï¼‰
        response = requests.get(base_url, params=params, timeout=30)
        response.raise_for_status()

        # è§£æ JSON å“åº”
        data = response.json()
        results = data.get("results", [])
        meta = data.get("meta", {})
        total_count = meta.get("count", 0)

        # æ ¼å¼åŒ–è¾“å‡ºç»“æœ
        if not results:
            output = f"âŒ æœªæ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„è®ºæ–‡ã€‚\nå»ºè®®ï¼šå°è¯•æ›´é€šç”¨çš„å…³é”®è¯æˆ–æ£€æŸ¥æ‹¼å†™ã€‚"
            logger.warning("[search_papers_openalex] æœªæ‰¾åˆ°ç»“æœ")
        else:
            # æ„å»ºæœç´¢æ‘˜è¦
            filter_desc = []
            if publication_year:
                filter_desc.append(f"å¹´ä»½: {publication_year}")
            if open_access_only:
                filter_desc.append("ä»…å¼€æ”¾è·å–")
            if cited_by_count_min:
                filter_desc.append(f"å¼•ç”¨â‰¥{cited_by_count_min}")

            filter_text = f" ({', '.join(filter_desc)})" if filter_desc else ""
            output = f"ğŸ“š æ‰¾åˆ° {len(results)} ç¯‡è®ºæ–‡{filter_text}\n"
            output += f"ğŸ“Š æ•°æ®åº“æ€»è®¡: {total_count:,} ç¯‡ç›¸å…³æ–‡çŒ®\n\n"

            for idx, work in enumerate(results, 1):
                # æå–åŸºæœ¬ä¿¡æ¯
                title = work.get("title") or work.get("display_name", "æœªçŸ¥æ ‡é¢˜")
                doi = work.get("doi", "")
                pub_year = work.get("publication_year", "")
                pub_date = work.get("publication_date", "")
                cited_count = work.get("cited_by_count", 0)
                openalex_id = work.get("id", "")
                work_type = work.get("type", "").replace("-", " ").title()

                # æå–ä½œè€…ä¿¡æ¯ï¼ˆåªæ˜¾ç¤ºå‰ 3 ä½ï¼‰
                authorships = work.get("authorships", [])
                if authorships:
                    author_names = [
                        auth.get("author", {}).get("display_name", "æœªçŸ¥")
                        for auth in authorships[:3]
                    ]
                    if len(authorships) > 3:
                        authors = (
                            ", ".join(author_names) + f" ç­‰ ({len(authorships)} ä½ä½œè€…)"
                        )
                    else:
                        authors = ", ".join(author_names)
                else:
                    authors = "æœªçŸ¥ä½œè€…"

                # æå–æœŸåˆŠ/æ¥æºä¿¡æ¯
                primary_location = work.get("primary_location") or {}
                source = primary_location.get("source") or {}
                source_name = source.get("display_name", "")

                # æå–å¼€æ”¾è·å–ä¿¡æ¯
                open_access = work.get("open_access") or {}
                oa_url = open_access.get("oa_url", "")
                oa_status = open_access.get("oa_status", "closed")
                is_oa = open_access.get("is_oa", False)

                # æå–ç ”ç©¶ä¸»é¢˜ï¼ˆå‰ 2 ä¸ªï¼‰
                topics = work.get("topics") or []
                topic_names = [
                    t.get("display_name", "")
                    for t in topics[:2]
                    if t.get("display_name")
                ]

                # æ„å»ºè¾“å‡º - ä½¿ç”¨æ›´æ¸…æ™°çš„æ ¼å¼
                output += f"## {idx}. {title}\n\n"

                # åŸºæœ¬ä¿¡æ¯
                output += f"**ğŸ‘¥ ä½œè€…:** {authors}\n"
                if source_name:
                    output += f"**ğŸ“– æ¥æº:** {source_name}\n"
                if pub_year:
                    output += f"**ğŸ“… å‘è¡¨:** {pub_year}"
                    if pub_date and pub_date != pub_year:
                        output += f" ({pub_date})"
                    output += "\n"
                if work_type:
                    output += f"**ğŸ“„ ç±»å‹:** {work_type}\n"

                # å½±å“åŠ›æŒ‡æ ‡
                output += f"**ğŸ“Š å¼•ç”¨æ¬¡æ•°:** {cited_count:,}\n"

                # ç ”ç©¶ä¸»é¢˜
                if topic_names:
                    output += f"**ğŸ”¬ ç ”ç©¶ä¸»é¢˜:** {', '.join(topic_names)}\n"

                # é“¾æ¥å’Œè®¿é—®
                if doi:
                    # ç§»é™¤ DOI URL å‰ç¼€ï¼Œåªä¿ç•™ DOI
                    doi_clean = doi.replace("https://doi.org/", "")
                    output += f"**ğŸ”— DOI:** [{doi_clean}]({doi})\n"

                # å¼€æ”¾è·å–çŠ¶æ€
                if is_oa and oa_url:
                    oa_emoji = "ğŸ”“"
                    oa_text = {
                        "gold": "é‡‘è‰²å¼€æ”¾è·å–",
                        "green": "ç»¿è‰²å¼€æ”¾è·å–",
                        "hybrid": "æ··åˆå¼€æ”¾è·å–",
                        "bronze": "é“œè‰²å¼€æ”¾è·å–",
                    }.get(oa_status, "å¼€æ”¾è·å–")
                    output += f"**{oa_emoji} å…¨æ–‡è®¿é—®:** [{oa_text}]({oa_url})\n"
                else:
                    output += "**ğŸ”’ è®¿é—®:** éœ€è¦è®¢é˜…æˆ–ä»˜è´¹\n"

                # OpenAlex é“¾æ¥
                if openalex_id:
                    output += f"**ğŸ” è¯¦æƒ…:** {openalex_id}\n"

                output += "\n"

            logger.info(f"[search_papers_openalex] æˆåŠŸè¿”å› {len(results)} ç¯‡è®ºæ–‡")

        return output

    except requests.exceptions.Timeout:
        error_msg = "â±ï¸ OpenAlex API è¯·æ±‚è¶…æ—¶ï¼ˆ>30ç§’ï¼‰\nå»ºè®®ï¼šè¯·ç¨åé‡è¯•ï¼Œæˆ–å°è¯•æ›´å…·ä½“çš„æœç´¢æ¡ä»¶ã€‚"
        logger.error("[search_papers_openalex] è¯·æ±‚è¶…æ—¶")
        return error_msg

    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 403:
            error_msg = "ğŸš« å·²è¾¾åˆ° API é€Ÿç‡é™åˆ¶\nå»ºè®®ï¼šè¯·ç­‰å¾… 1 åˆ†é’Ÿåé‡è¯•ã€‚"
        elif e.response.status_code == 404:
            error_msg = "âŒ æœªæ‰¾åˆ°èµ„æº\nå»ºè®®ï¼šæ£€æŸ¥æœç´¢å‚æ•°æ˜¯å¦æ­£ç¡®ã€‚"
        elif e.response.status_code >= 500:
            error_msg = f"âš ï¸ OpenAlex æœåŠ¡å™¨é”™è¯¯ ({e.response.status_code})\nå»ºè®®ï¼šè¿™æ˜¯ä¸´æ—¶é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        else:
            error_msg = f"âŒ API è¯·æ±‚å¤±è´¥: HTTP {e.response.status_code}\n{str(e)}"
        logger.error(f"[search_papers_openalex] HTTPé”™è¯¯: {e.response.status_code}")
        return error_msg

    except requests.exceptions.RequestException as e:
        error_msg = f"ğŸŒ ç½‘ç»œè¿æ¥é”™è¯¯: {str(e)}\nå»ºè®®ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚"
        logger.error(f"[search_papers_openalex] ç½‘ç»œé”™è¯¯: {str(e)}")
        return error_msg

    except Exception as e:
        error_msg = f"âŒ æœç´¢è¿‡ç¨‹å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}\nå»ºè®®ï¼šè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚"
        logger.error("[search_papers_openalex] æœªçŸ¥é”™è¯¯")
        logger.exception(e)
        return error_msg


# ========== Semantic Scholar æœç´¢å·¥å…· ==========


class SemanticScholarSearchInput(BaseModel):
    """Semantic Scholar è®ºæ–‡æœç´¢çš„è¾“å…¥å‚æ•°
    
    Semantic Scholar æ˜¯ç”±è‰¾ä¼¦äººå·¥æ™ºèƒ½ç ”ç©¶æ‰€å¼€å‘çš„å…è´¹å­¦æœ¯æœç´¢å¼•æ“ï¼Œ
    æ‹¥æœ‰è¶…è¿‡ 2 äº¿ç¯‡è®ºæ–‡ï¼Œç‰¹åˆ«æ“…é•¿è®¡ç®—æœºç§‘å­¦å’Œç¥ç»ç§‘å­¦é¢†åŸŸã€‚
    å®ƒä½¿ç”¨ AI æŠ€æœ¯æå–è®ºæ–‡çš„å…³é”®ä¿¡æ¯å’Œå½±å“åŠ›æŒ‡æ ‡ã€‚
    """

    query: str = Field(
        description=(
            "æœç´¢å…³é”®è¯æˆ–çŸ­è¯­ï¼Œä¼šåœ¨è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦ä¸­æœç´¢ã€‚"
            "\n\n**ç”¨æˆ·å¸¸è§è¯´æ³•æ˜ å°„ï¼š**"
            "\n- 'æ‰¾å…³äº...çš„è®ºæ–‡' â†’ ç›´æ¥ä½¿ç”¨ä¸»é¢˜è¯ä½œä¸º query"
            "\n- 'æœç´¢ transformer æ¨¡å‹' â†’ query='transformer models'"
            "\n- 'æŸ¥æ‰¾æ·±åº¦å­¦ä¹ çš„æœ€æ–°ç ”ç©¶' â†’ query='deep learning'"
            "\n\n**ç¤ºä¾‹ï¼š**"
            "\n- 'neural networks'"
            "\n- 'reinforcement learning'"
            "\n- 'computer vision'"
        )
    )

    max_results: int = Field(
        default=10,
        description=(
            "è¿”å›çš„æœ€å¤§è®ºæ–‡æ•°é‡ï¼Œé»˜è®¤ä¸º 10 ç¯‡ã€‚"
            "\n\n**ç”¨æˆ·å¸¸è§è¯´æ³•æ˜ å°„ï¼š**"
            "\n- 'æ‰¾å‡ ç¯‡è®ºæ–‡' / 'ç»™æˆ‘ä¸€äº›' â†’ 5-10"
            "\n- 'è¯¦ç»†æœç´¢' / 'å…¨é¢æœç´¢' â†’ 15-20"
            "\n- 'å¿«é€Ÿçœ‹çœ‹' â†’ 3-5"
        ),
        ge=1,
        le=100,
    )

    year_filter: Optional[str] = Field(
        default=None,
        description=(
            "å‘è¡¨å¹´ä»½ç­›é€‰ã€‚æ”¯æŒå•å¹´ä»½ã€å¹´ä»½èŒƒå›´æˆ–ç‰¹å®šåŒºé—´ã€‚"
            "\n\n**æ ¼å¼ï¼š**"
            "\n- å•å¹´ä»½: '2023'"
            "\n- å¹´ä»½èŒƒå›´: '2020-2023'"
            "\n- æœ€è¿‘å¹´ä»½: '2022-' (2022å¹´è‡³ä»Š)"
            "\n\n**ç”¨æˆ·å¸¸è§è¯´æ³•æ˜ å°„ï¼š**"
            "\n- 'æœ€è¿‘çš„' / 'æœ€æ–°çš„' â†’ ä½¿ç”¨å½“å‰å¹´ä»½ï¼Œå¦‚ '2024-'"
            "\n- 'è¿‘å‡ å¹´' / 'æœ€è¿‘å‡ å¹´' â†’ '2020-2024'"
            "\n- '2023å¹´çš„' â†’ '2023'"
        ),
    )

    min_citation_count: Optional[int] = Field(
        default=None,
        description=(
            "æœ€å°å¼•ç”¨æ¬¡æ•°ã€‚åªè¿”å›å¼•ç”¨æ¬¡æ•°ä¸å°‘äºæ­¤å€¼çš„è®ºæ–‡ã€‚"
            "\n\n**ç”¨æˆ·å¸¸è§è¯´æ³•æ˜ å°„ï¼š**"
            "\n- 'æœ‰å½±å“åŠ›çš„' / 'é‡è¦çš„' â†’ 50-100"
            "\n- 'ç»å…¸è®ºæ–‡' / 'é«˜å¼•ç”¨' â†’ 100+"
            "\n- 'è¢«å¹¿æ³›å¼•ç”¨' â†’ 200+"
        ),
        ge=0,
    )

    fields_of_study: Optional[str] = Field(
        default=None,
        description=(
            "å­¦ç§‘é¢†åŸŸç­›é€‰ã€‚ç”¨äºé™åˆ¶æœç´¢èŒƒå›´åˆ°ç‰¹å®šå­¦ç§‘ã€‚"
            "\n\n**å¸¸è§å­¦ç§‘é¢†åŸŸï¼š**"
            "\n- 'Computer Science' (è®¡ç®—æœºç§‘å­¦)"
            "\n- 'Medicine' (åŒ»å­¦)"
            "\n- 'Biology' (ç”Ÿç‰©å­¦)"
            "\n- 'Physics' (ç‰©ç†å­¦)"
            "\n- 'Mathematics' (æ•°å­¦)"
            "\n- 'Engineering' (å·¥ç¨‹å­¦)"
            "\n- 'Psychology' (å¿ƒç†å­¦)"
            "\n\n**ç”¨æˆ·å¸¸è§è¯´æ³•æ˜ å°„ï¼š**"
            "\n- 'è®¡ç®—æœºé¢†åŸŸ' â†’ 'Computer Science'"
            "\n- 'åŒ»å­¦ç›¸å…³' â†’ 'Medicine'"
            "\n- 'ç‰©ç†æ–¹å‘' â†’ 'Physics'"
        ),
    )

    sort: Literal["relevance", "citationCount", "publicationDate"] = Field(
        default="relevance",
        description=(
            "ç»“æœæ’åºæ–¹å¼ã€‚"
            "\n\n**é€‰é¡¹ï¼š**"
            "\n- 'relevance': ç›¸å…³æ€§æ’åºï¼ˆé»˜è®¤ï¼Œæœ€åŒ¹é…çš„è®ºæ–‡åœ¨å‰ï¼‰"
            "\n- 'citationCount': æŒ‰å¼•ç”¨æ¬¡æ•°é™åºï¼ˆæœ€å¤šå¼•ç”¨çš„åœ¨å‰ï¼‰"
            "\n- 'publicationDate': æŒ‰å‘è¡¨æ—¥æœŸé™åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰"
            "\n\n**ç”¨æˆ·å¸¸è§è¯´æ³•æ˜ å°„ï¼š**"
            "\n- 'æœ€ç›¸å…³çš„' / 'æœ€åŒ¹é…çš„' â†’ 'relevance'"
            "\n- 'æœ€æœ‰å½±å“åŠ›çš„' / 'è¢«å¼•æœ€å¤šçš„' â†’ 'citationCount'"
            "\n- 'æœ€æ–°çš„' / 'æœ€è¿‘å‘è¡¨çš„' â†’ 'publicationDate'"
        ),
    )

    open_access_only: bool = Field(
        default=False,
        description=(
            "æ˜¯å¦åªè¿”å›å¼€æ”¾è·å–ï¼ˆOpen Accessï¼‰çš„è®ºæ–‡ã€‚"
            "\n\n**ç”¨æˆ·å¸¸è§è¯´æ³•æ˜ å°„ï¼š**"
            "\n- 'èƒ½å…è´¹ä¸‹è½½çš„' / 'å…è´¹è®ºæ–‡' â†’ True"
            "\n- 'å¼€æ”¾è·å–' / 'OAè®ºæ–‡' â†’ True"
            "\n- 'æˆ‘èƒ½ç›´æ¥çœ‹çš„' â†’ True"
        ),
    )


@tool(args_schema=SemanticScholarSearchInput)
def search_papers_semantic_scholar(
    query: str,
    max_results: int = 10,
    year_filter: Optional[str] = None,
    min_citation_count: Optional[int] = None,
    fields_of_study: Optional[str] = None,
    sort: Literal["relevance", "citationCount", "publicationDate"] = "relevance",
    open_access_only: bool = False,
) -> str:
    """æœç´¢ Semantic Scholar å­¦æœ¯è®ºæ–‡æ•°æ®åº“ - 200M+ ç¯‡è®ºæ–‡ï¼ŒAI é©±åŠ¨çš„å­¦æœ¯æœç´¢ã€‚

    Semantic Scholar æ˜¯ç”±è‰¾ä¼¦äººå·¥æ™ºèƒ½ç ”ç©¶æ‰€ï¼ˆAI2ï¼‰å¼€å‘çš„å…è´¹å­¦æœ¯æœç´¢å¼•æ“ï¼Œ
    ä½¿ç”¨ AI æŠ€æœ¯åˆ†æå’Œç†è§£è®ºæ–‡å†…å®¹ã€‚ç‰¹åˆ«æ“…é•¿è®¡ç®—æœºç§‘å­¦ã€ç¥ç»ç§‘å­¦ç­‰é¢†åŸŸï¼Œ
    æä¾›ä¸°å¯Œçš„å¼•ç”¨å…³ç³»ã€å½±å“åŠ›æŒ‡æ ‡å’Œè®ºæ–‡æ‘˜è¦ã€‚

    **é€‚ç”¨åœºæ™¯ï¼š**
    - è®¡ç®—æœºç§‘å­¦å’Œ AI é¢†åŸŸçš„è®ºæ–‡æœç´¢ï¼ˆè¦†ç›–æœ€å…¨ï¼‰
    - éœ€è¦è¯¦ç»†å¼•ç”¨åˆ†æå’Œå½±å“åŠ›æŒ‡æ ‡
    - æŸ¥æ‰¾æœ‰å½±å“åŠ›çš„ç»å…¸è®ºæ–‡
    - è·Ÿè¸ªæœ€æ–°ç ”ç©¶è¿›å±•

    **ä¸å…¶ä»–å·¥å…·çš„åŒºåˆ«ï¼š**
    - vs ArXiv: Semantic Scholar åŒ…å«å·²å‘è¡¨è®ºæ–‡ï¼Œæœ‰å¼•ç”¨æ•°æ®
    - vs OpenAlex: Semantic Scholar åœ¨ CS/AI é¢†åŸŸæ›´ä¸“ä¸šï¼ŒUI æ›´å‹å¥½

    Args:
        query: æœç´¢å…³é”®è¯ï¼Œåœ¨æ ‡é¢˜å’Œæ‘˜è¦ä¸­æœç´¢
        max_results: æœ€å¤šè¿”å›ç»“æœæ•°ï¼Œé»˜è®¤ 10 ç¯‡
        year_filter: å¹´ä»½ç­›é€‰ï¼Œå¦‚ "2023" æˆ– "2020-2023"
        min_citation_count: æœ€å°å¼•ç”¨æ¬¡æ•°ç­›é€‰
        fields_of_study: å­¦ç§‘é¢†åŸŸç­›é€‰ï¼Œå¦‚ "Computer Science"
        sort: æ’åºæ–¹å¼ï¼ˆrelevance/citationCount/publicationDateï¼‰
        open_access_only: æ˜¯å¦åªè¿”å›å¼€æ”¾è·å–è®ºæ–‡
    """
    logger.info(
        f"[search_papers_semantic_scholar] æœç´¢è®ºæ–‡ - "
        f"æŸ¥è¯¢: {query}, æ•°é‡: {max_results}, æ’åº: {sort}"
    )
    logger.info(
        f"[search_papers_semantic_scholar] æœç´¢å‚æ•°: {{"
        f"'query': '{query}', "
        f"'max_results': {max_results}, "
        f"'year_filter': '{year_filter}', "
        f"'min_citation_count': {min_citation_count}, "
        f"'fields_of_study': '{fields_of_study}', "
        f"'sort': '{sort}', "
        f"'open_access_only': {open_access_only}"
        f"}}"
    )

    try:
        # æ„å»º API è¯·æ±‚
        base_url = "https://api.semanticscholar.org/graph/v1/paper/search"

        # è¯·æ±‚çš„å­—æ®µï¼ˆæ ¹æ® API æ–‡æ¡£é€‰æ‹©éœ€è¦çš„å­—æ®µï¼‰
        fields = [
            "paperId",
            "title",
            "abstract",
            "authors",
            "year",
            "citationCount",
            "influentialCitationCount",
            "venue",
            "publicationDate",
            "publicationTypes",
            "fieldsOfStudy",
            "url",
            "openAccessPdf",
            "externalIds",
        ]

        params = {
            "query": query,
            "limit": max_results,
            "fields": ",".join(fields),
        }

        # æ·»åŠ å¹´ä»½ç­›é€‰
        if year_filter:
            # æ”¯æŒ "2023" æˆ– "2020-2023" æ ¼å¼
            if "-" in year_filter:
                parts = year_filter.split("-")
                if parts[0]:  # èµ·å§‹å¹´ä»½
                    params["year"] = f"{parts[0]}-"
                if len(parts) > 1 and parts[1]:  # ç»“æŸå¹´ä»½
                    params["year"] = f"{parts[0]}-{parts[1]}"
            else:
                params["year"] = year_filter

        # æ·»åŠ æœ€å°å¼•ç”¨æ¬¡æ•°ç­›é€‰
        if min_citation_count is not None:
            params["minCitationCount"] = min_citation_count

        # æ·»åŠ å­¦ç§‘é¢†åŸŸç­›é€‰
        if fields_of_study:
            params["fieldsOfStudy"] = fields_of_study

        # æ·»åŠ å¼€æ”¾è·å–ç­›é€‰
        if open_access_only:
            params["openAccessPdf"] = ""  # åªè¿”å›æœ‰ OA PDF çš„è®ºæ–‡

        # è®¾ç½®æ’åº
        # Semantic Scholar API ä¸ç›´æ¥æ”¯æŒ sort å‚æ•°ï¼Œä½†æˆ‘ä»¬å¯ä»¥åœ¨å®¢æˆ·ç«¯æ’åº
        # å…ˆè·å–æ•°æ®ï¼Œç„¶åæ’åº

        logger.debug(f"[search_papers_semantic_scholar] API å‚æ•°: {params}")

        # å‘é€è¯·æ±‚ï¼ˆä¸éœ€è¦ API keyï¼Œä½†å»ºè®®è®¾ç½® User-Agentï¼‰
        headers = {
            "User-Agent": "ScholarMate/1.0 (mailto:347699233@qq.com)",
        }

        response = requests.get(base_url, params=params, headers=headers, timeout=30)
        response.raise_for_status()

        # è§£æå“åº”
        data = response.json()
        papers = data.get("data", [])
        total = data.get("total", 0)

        if not papers:
            return (
                "ğŸ“­ æœªæ‰¾åˆ°åŒ¹é…çš„è®ºæ–‡\n\n"
                f"**æœç´¢å…³é”®è¯ï¼š** {query}\n\n"
                "**å»ºè®®ï¼š**\n"
                "- å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯\n"
                "- å‡å°‘ç­›é€‰æ¡ä»¶\n"
                "- ä½¿ç”¨æ›´é€šç”¨çš„æœ¯è¯­"
            )

        # å®¢æˆ·ç«¯æ’åºï¼ˆå¦‚æœéœ€è¦ï¼‰
        if sort == "citationCount":
            papers = sorted(
                papers, key=lambda x: x.get("citationCount", 0), reverse=True
            )
        elif sort == "publicationDate":
            papers = sorted(
                papers,
                key=lambda x: x.get("publicationDate", "") or "",
                reverse=True,
            )

        # æ„å»ºè¾“å‡º
        output = f"# ğŸ“š æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡ï¼ˆå…± {total:,} ç¯‡åŒ¹é…ï¼‰\n\n"
        output += f"**æœç´¢å…³é”®è¯ï¼š** {query}\n"

        if year_filter:
            output += f"**å¹´ä»½ç­›é€‰ï¼š** {year_filter}\n"
        if min_citation_count:
            output += f"**æœ€å°å¼•ç”¨ï¼š** {min_citation_count}\n"
        if fields_of_study:
            output += f"**å­¦ç§‘é¢†åŸŸï¼š** {fields_of_study}\n"

        output += f"**æ’åºæ–¹å¼ï¼š** {sort}\n\n"
        output += "---\n\n"

        # æ ¼å¼åŒ–æ¯ç¯‡è®ºæ–‡
        for idx, paper in enumerate(papers, 1):
            title = paper.get("title", "æœªçŸ¥æ ‡é¢˜")
            abstract = paper.get("abstract", "")
            year = paper.get("year") or "æœªçŸ¥"
            citation_count = paper.get("citationCount", 0)
            influential_citations = paper.get("influentialCitationCount", 0)
            venue = paper.get("venue", "")
            url = paper.get("url", "")
            publication_date = paper.get("publicationDate", "")

            # ä½œè€…ä¿¡æ¯
            authors_data = paper.get("authors") or []
            if authors_data and len(authors_data) > 0:
                author_names = [a.get("name", "") for a in authors_data if a.get("name")]
                if len(author_names) > 4:
                    authors = ", ".join(author_names[:4]) + f" ç­‰ ({len(author_names)} ä½ä½œè€…)"
                else:
                    authors = ", ".join(author_names) if author_names else "æœªçŸ¥ä½œè€…"
            else:
                authors = "æœªçŸ¥ä½œè€…"

            # å¼€æ”¾è·å–ä¿¡æ¯
            oa_pdf = paper.get("openAccessPdf")
            if oa_pdf and oa_pdf.get("url"):
                oa_icon = "ğŸŸ¢"
                oa_text = f"å¼€æ”¾è·å– - [ä¸‹è½½ PDF]({oa_pdf['url']})"
            else:
                oa_icon = "ğŸ”’"
                oa_text = "éœ€è®¢é˜…"

            # å¤–éƒ¨é“¾æ¥ï¼ˆDOI, ArXiv ç­‰ï¼‰
            external_ids = paper.get("externalIds") or {}
            doi = external_ids.get("DOI")
            arxiv = external_ids.get("ArXiv")

            # å­¦ç§‘é¢†åŸŸ
            fields = paper.get("fieldsOfStudy") or []
            fields_text = ", ".join(fields[:3]) if fields else ""

            # æ„å»ºè¾“å‡º
            output += f"## {idx}. {title}\n\n"

            # åŸºæœ¬ä¿¡æ¯
            output += f"**ğŸ‘¥ ä½œè€…ï¼š** {authors}\n\n"
            output += f"**ğŸ“… å‘è¡¨ï¼š** {year}"
            if publication_date:
                output += f" ({publication_date})"
            output += "\n\n"

            if venue:
                output += f"**ğŸ“– å‘è¡¨äºï¼š** {venue}\n\n"

            # å¼•ç”¨ä¿¡æ¯
            output += (
                f"**ğŸ“Š å¼•ç”¨æ¬¡æ•°ï¼š** {citation_count:,} "
                f"ï¼ˆå…¶ä¸­æœ‰å½±å“åŠ›çš„å¼•ç”¨ï¼š{influential_citations}ï¼‰\n\n"
            )

            # å­¦ç§‘é¢†åŸŸ
            if fields_text:
                output += f"**ğŸ·ï¸ å­¦ç§‘é¢†åŸŸï¼š** {fields_text}\n\n"

            # å¼€æ”¾è·å–çŠ¶æ€
            output += f"**{oa_icon} è®¿é—®ï¼š** {oa_text}\n\n"

            # é“¾æ¥
            output += "**ğŸ”— é“¾æ¥ï¼š**\n"
            if url:
                output += f"- [Semantic Scholar]({url})\n"
            if doi:
                output += f"- [DOI](https://doi.org/{doi})\n"
            if arxiv:
                output += f"- [ArXiv](https://arxiv.org/abs/{arxiv})\n"
            output += "\n"

            # æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
            if abstract:
                # é™åˆ¶æ‘˜è¦é•¿åº¦
                if len(abstract) > 400:
                    abstract = abstract[:400] + "..."
                output += f"**ğŸ“ æ‘˜è¦ï¼š**\n{abstract}\n\n"

            output += "---\n\n"

        # æ·»åŠ æç¤ºä¿¡æ¯
        output += "\nğŸ’¡ **æç¤ºï¼š** æ•°æ®æ¥è‡ª Semantic Scholar API\n"
        output += "- ğŸŸ¢ ç»¿è‰²è¡¨ç¤ºå¯å…è´¹è·å–å…¨æ–‡\n"
        output += "- å¼•ç”¨æ¬¡æ•°åæ˜ è®ºæ–‡å½±å“åŠ›\n"
        output += "- æœ‰å½±å“åŠ›çš„å¼•ç”¨ï¼šè¢«é‡è¦è®ºæ–‡å¼•ç”¨çš„æ¬¡æ•°\n"

        return output

    except requests.exceptions.Timeout:
        error_msg = (
            "â±ï¸ Semantic Scholar API è¯·æ±‚è¶…æ—¶\n"
            "å»ºè®®ï¼šç½‘ç»œè¾ƒæ…¢æˆ–æœåŠ¡ç¹å¿™ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        )
        logger.error("[search_papers_semantic_scholar] è¯·æ±‚è¶…æ—¶")
        return error_msg

    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 429:
            error_msg = (
                "ğŸš« å·²è¾¾åˆ° API é€Ÿç‡é™åˆ¶\n"
                "Semantic Scholar å…è´¹ API é™åˆ¶ï¼š100 requests/5 minutes\n"
                "å»ºè®®ï¼šè¯·ç­‰å¾…å‡ åˆ†é’Ÿåé‡è¯•ã€‚"
            )
        elif e.response.status_code == 400:
            error_msg = (
                "âŒ è¯·æ±‚å‚æ•°é”™è¯¯\n"
                "å»ºè®®ï¼šæ£€æŸ¥æœç´¢å‚æ•°æ˜¯å¦æ­£ç¡®ï¼ˆå¦‚å¹´ä»½æ ¼å¼ã€å­¦ç§‘é¢†åŸŸåç§°ç­‰ï¼‰ã€‚"
            )
        elif e.response.status_code >= 500:
            error_msg = (
                f"âš ï¸ Semantic Scholar æœåŠ¡å™¨é”™è¯¯ ({e.response.status_code})\n"
                f"å»ºè®®ï¼šè¿™æ˜¯ä¸´æ—¶é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"
            )
        else:
            error_msg = f"âŒ API è¯·æ±‚å¤±è´¥: HTTP {e.response.status_code}\n{str(e)}"
        logger.error(
            f"[search_papers_semantic_scholar] HTTPé”™è¯¯: {e.response.status_code}"
        )
        return error_msg

    except requests.exceptions.RequestException as e:
        error_msg = f"ğŸŒ ç½‘ç»œè¿æ¥é”™è¯¯: {str(e)}\nå»ºè®®ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚"
        logger.error(f"[search_papers_semantic_scholar] ç½‘ç»œé”™è¯¯: {str(e)}")
        return error_msg

    except Exception as e:
        error_msg = f"âŒ æœç´¢è¿‡ç¨‹å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}\nå»ºè®®ï¼šè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚"
        logger.error("[search_papers_semantic_scholar] æœªçŸ¥é”™è¯¯")
        logger.exception(e)
        return error_msg


# ========== å¤šæºèšåˆæœç´¢å·¥å…· ==========


def normalize_title(title: str) -> str:
    """æ ‡å‡†åŒ–è®ºæ–‡æ ‡é¢˜ç”¨äºå»é‡
    
    - è½¬æ¢ä¸ºå°å†™
    - ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
    - ç§»é™¤å¤šä½™ç©ºæ ¼
    """
    if not title:
        return ""
    # è½¬å°å†™
    title = title.lower()
    # ç§»é™¤æ ‡ç‚¹å’Œç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼
    title = re.sub(r'[^\w\s]', '', title)
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    title = ' '.join(title.split())
    return title


def title_similarity(title1: str, title2: str) -> float:
    """è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ä¹‹é—´ï¼‰"""
    if not title1 or not title2:
        return 0.0
    norm1 = normalize_title(title1)
    norm2 = normalize_title(title2)
    return SequenceMatcher(None, norm1, norm2).ratio()


def extract_paper_data_from_text(text_result: str, source: str) -> List[Dict[str, Any]]:
    """ä»å·¥å…·è¿”å›çš„æ–‡æœ¬ç»“æœä¸­æå–ç»“æ„åŒ–è®ºæ–‡æ•°æ®
    
    è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œä¸»è¦æå–å…³é”®ä¿¡æ¯ç”¨äºå»é‡ã€‚
    å®é™…ä½¿ç”¨ä¸­ï¼Œæˆ‘ä»¬ä¼šç›´æ¥è°ƒç”¨åº•å±‚å‡½æ•°è·å–ç»“æ„åŒ–æ•°æ®ã€‚
    """
    papers = []
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–è®ºæ–‡æ ‡é¢˜ï¼ˆä»¥ ## å¼€å¤´çš„è¡Œï¼‰
    title_pattern = r'## \d+\.\s+(.+?)(?=\n|$)'
    titles = re.findall(title_pattern, text_result)
    
    # æå– DOIï¼ˆç®€åŒ–ç‰ˆï¼‰
    doi_pattern = r'\[DOI\]\(https://doi\.org/([^\)]+)\)'
    dois = re.findall(doi_pattern, text_result)
    
    # æå– ArXiv ID
    arxiv_pattern = r'\[ArXiv\]\(https://arxiv\.org/abs/([^\)]+)\)'
    arxiv_ids = re.findall(arxiv_pattern, text_result)
    
    for i, title in enumerate(titles):
        paper = {
            'title': title.strip(),
            'source': source,
            'doi': dois[i] if i < len(dois) else None,
            'arxiv_id': arxiv_ids[i] if i < len(arxiv_ids) else None,
        }
        papers.append(paper)
    
    return papers


def safe_search(search_func, *args, **kwargs) -> tuple[str, List[Dict[str, Any]]]:
    """å®‰å…¨åœ°è°ƒç”¨æœç´¢å‡½æ•°ï¼Œæ•è·å¼‚å¸¸
    
    Returns:
        (text_result, structured_data)
    """
    try:
        result = search_func(*args, **kwargs)
        # å¦‚æœå‡½æ•°è¿”å›å­—ç¬¦ä¸²ï¼ˆå½“å‰çš„å®ç°ï¼‰
        if isinstance(result, str):
            return result, []
        # å¦‚æœå‡½æ•°è¿”å›ç»“æ„åŒ–æ•°æ®ï¼ˆæœªæ¥å¯èƒ½çš„å®ç°ï¼‰
        return str(result), []
    except Exception as e:
        logger.warning(f"æœç´¢å‡½æ•° {search_func.__name__} å¤±è´¥: {str(e)}")
        return f"âŒ {search_func.__name__} æŸ¥è¯¢å¤±è´¥: {str(e)}", []


def deduplicate_papers(papers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """å¯¹è®ºæ–‡åˆ—è¡¨è¿›è¡Œå»é‡
    
    å»é‡ç­–ç•¥ï¼š
    1. ä¼˜å…ˆä½¿ç”¨ DOI
    2. å…¶æ¬¡ä½¿ç”¨ ArXiv ID
    3. æœ€åä½¿ç”¨æ ‡é¢˜ç›¸ä¼¼åº¦ï¼ˆ>90% è§†ä¸ºé‡å¤ï¼‰
    """
    if not papers:
        return []
    
    unique_papers = []
    seen_dois = set()
    seen_arxiv_ids = set()
    seen_titles = []
    
    for paper in papers:
        is_duplicate = False
        
        # æ£€æŸ¥ DOI
        doi = paper.get('doi')
        if doi:
            if doi in seen_dois:
                is_duplicate = True
            else:
                seen_dois.add(doi)
        
        # æ£€æŸ¥ ArXiv ID
        if not is_duplicate:
            arxiv_id = paper.get('arxiv_id')
            if arxiv_id:
                if arxiv_id in seen_arxiv_ids:
                    is_duplicate = True
                else:
                    seen_arxiv_ids.add(arxiv_id)
        
        # æ£€æŸ¥æ ‡é¢˜ç›¸ä¼¼åº¦
        if not is_duplicate:
            title = paper.get('title', '')
            for seen_title in seen_titles:
                if title_similarity(title, seen_title) > 0.9:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                seen_titles.append(title)
        
        if not is_duplicate:
            unique_papers.append(paper)
    
    return unique_papers


class AggregatedSearchInput(BaseModel):
    """å¤šæºèšåˆæœç´¢çš„è¾“å…¥å‚æ•°"""

    query: str = Field(
        description=(
            "æœç´¢å…³é”®è¯æˆ–çŸ­è¯­ã€‚"
            "\n\n**ç”¨æˆ·å¸¸è§è¯´æ³•æ˜ å°„ï¼š**"
            "\n- 'å…¨é¢æœç´¢...çš„è®ºæ–‡' â†’ ä½¿ç”¨èšåˆæœç´¢"
            "\n- 'æ‰¾æ‰€æœ‰å…³äº...çš„ç ”ç©¶' â†’ ä½¿ç”¨èšåˆæœç´¢"
            "\n- 'å¤šä¸ªæ•°æ®åº“æœç´¢' â†’ ä½¿ç”¨èšåˆæœç´¢"
        )
    )

    max_results_per_source: int = Field(
        default=5,
        description=(
            "æ¯ä¸ªæ•°æ®æºè¿”å›çš„æœ€å¤§è®ºæ–‡æ•°é‡ï¼Œé»˜è®¤ 5 ç¯‡ã€‚"
            "\nèšåˆåæ€»æ•°å¯èƒ½è¾¾åˆ° 15 ç¯‡ï¼ˆ3ä¸ªæºÃ—5ç¯‡ï¼‰ï¼Œå»é‡åä¼šæ›´å°‘ã€‚"
        ),
        ge=1,
        le=20,
    )

    sources: List[Literal["openalex", "arxiv", "semantic_scholar"]] = Field(
        default=["openalex", "arxiv", "semantic_scholar"],
        description=(
            "è¦æŸ¥è¯¢çš„æ•°æ®æºåˆ—è¡¨ã€‚"
            "\n\n**é€‰é¡¹ï¼š**"
            "\n- 'openalex': OpenAlexï¼ˆæœ€å…¨é¢ï¼‰"
            "\n- 'arxiv': ArXivï¼ˆé¢„å°æœ¬ï¼‰"
            "\n- 'semantic_scholar': Semantic Scholarï¼ˆå¼•ç”¨åˆ†æï¼‰"
            "\n\n**ç”¨æˆ·å¸¸è§è¯´æ³•æ˜ å°„ï¼š**"
            "\n- 'æ‰€æœ‰æ•°æ®åº“' â†’ ['openalex', 'arxiv', 'semantic_scholar']"
            "\n- 'ArXiv å’Œ Semantic Scholar' â†’ ['arxiv', 'semantic_scholar']"
            "\n- 'åªæŸ¥ OpenAlex' â†’ ['openalex']"
        ),
    )

    deduplicate: bool = Field(
        default=True,
        description="æ˜¯å¦å¯¹ç»“æœå»é‡ã€‚å»ºè®®ä¿æŒä¸º Trueã€‚"
    )

    timeout_per_source: int = Field(
        default=30,
        description="æ¯ä¸ªæ•°æ®æºçš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 30 ç§’",
        ge=10,
        le=60,
    )


@tool(args_schema=AggregatedSearchInput)
def search_papers_aggregated(
    query: str,
    max_results_per_source: int = 5,
    sources: List[Literal["openalex", "arxiv", "semantic_scholar"]] = None,
    deduplicate: bool = True,
    timeout_per_source: int = 30,
) -> str:
    """å¤šæºèšåˆæœç´¢ - åŒæ—¶æŸ¥è¯¢å¤šä¸ªå­¦æœ¯æ•°æ®åº“å¹¶åˆå¹¶ç»“æœã€‚

    è¿™ä¸ªå·¥å…·ä¼šå¹¶è¡ŒæŸ¥è¯¢å¤šä¸ªå­¦æœ¯æ•°æ®åº“ï¼ˆOpenAlexã€ArXivã€Semantic Scholarï¼‰ï¼Œ
    ç„¶åæ™ºèƒ½å»é‡å’Œåˆå¹¶ç»“æœï¼Œæä¾›æœ€å…¨é¢çš„è®ºæ–‡æœç´¢ç»“æœã€‚

    **é€‚ç”¨åœºæ™¯ï¼š**
    - ç”¨æˆ·æ˜ç¡®è¦æ±‚"å…¨é¢æœç´¢"ã€"å¤šä¸ªæ•°æ®åº“"
    - éœ€è¦æœ€å¤§åŒ–æŸ¥å…¨ç‡
    - å¯¹æŸä¸ªä¸»é¢˜è¿›è¡Œå…¨é¢æ–‡çŒ®è°ƒç ”
    - æŸ¥æ‰¾å†·é—¨æˆ–æ–°å…´ä¸»é¢˜çš„è®ºæ–‡

    **ä¼˜åŠ¿ï¼š**
    - è¦†ç›–æ›´å…¨é¢ï¼ˆä¸‰ä¸ªæ•°æ®æºçš„å¹¶é›†ï¼‰
    - äº’è¡¥æ€§å¼ºï¼ˆæŸä¸ªæºæ²¡æœ‰çš„è®ºæ–‡ï¼Œå…¶ä»–æºå¯èƒ½æœ‰ï¼‰
    - å¹¶è¡ŒæŸ¥è¯¢ï¼Œé€Ÿåº¦è¾ƒå¿«
    - è‡ªåŠ¨å»é‡ï¼Œé¿å…é‡å¤ç»“æœ

    **æ³¨æ„äº‹é¡¹ï¼š**
    - æŸ¥è¯¢æ—¶é—´æ¯”å•æºç¨é•¿ï¼ˆçº¦3-5ç§’ï¼‰
    - å¦‚æœåªéœ€è¦å¿«é€Ÿç»“æœï¼Œå»ºè®®ä½¿ç”¨å•ä¸€æ•°æ®æºå·¥å…·

    Args:
        query: æœç´¢å…³é”®è¯
        max_results_per_source: æ¯ä¸ªæºè¿”å›çš„æœ€å¤§ç»“æœæ•°ï¼Œé»˜è®¤ 5
        sources: è¦æŸ¥è¯¢çš„æ•°æ®æºåˆ—è¡¨ï¼Œé»˜è®¤æŸ¥è¯¢æ‰€æœ‰ä¸‰ä¸ªæº
        deduplicate: æ˜¯å¦å»é‡ï¼Œé»˜è®¤ True
        timeout_per_source: æ¯ä¸ªæºçš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 30
    """
    if sources is None:
        sources = ["openalex", "arxiv", "semantic_scholar"]

    logger.info(
        f"[search_papers_aggregated] å¼€å§‹å¤šæºèšåˆæœç´¢ - "
        f"æŸ¥è¯¢: {query}, æ•°æ®æº: {sources}, æ¯æºç»“æœæ•°: {max_results_per_source}"
    )

    # è·å–åº•å±‚å‡½æ•°ï¼ˆä¸ä½¿ç”¨ @tool åŒ…è£…çš„ç‰ˆæœ¬ï¼‰
    # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦è®¿é—® .func å±æ€§æ¥è·å–åŸå§‹å‡½æ•°
    source_functions = {
        "openalex": search_papers_openalex.func if hasattr(search_papers_openalex, 'func') else search_papers_openalex,
        "arxiv": search_papers_ArXiv.func if hasattr(search_papers_ArXiv, 'func') else search_papers_ArXiv,
        "semantic_scholar": search_papers_semantic_scholar.func if hasattr(search_papers_semantic_scholar, 'func') else search_papers_semantic_scholar,
    }

    # å¹¶è¡ŒæŸ¥è¯¢æ‰€æœ‰æ•°æ®æº
    results = {}
    failed_sources = []

    with ThreadPoolExecutor(max_workers=len(sources)) as executor:
        # æäº¤æ‰€æœ‰æŸ¥è¯¢ä»»åŠ¡
        future_to_source = {}
        for source in sources:
            if source in source_functions:
                func = source_functions[source]
                future = executor.submit(
                    safe_search,
                    func,
                    query,
                    max_results_per_source
                )
                future_to_source[future] = source

        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_source, timeout=timeout_per_source + 5):
            source = future_to_source[future]
            try:
                text_result, structured_data = future.result(timeout=timeout_per_source)
                results[source] = text_result
                logger.info(f"[search_papers_aggregated] {source} æŸ¥è¯¢å®Œæˆ")
            except Exception as e:
                logger.error(f"[search_papers_aggregated] {source} æŸ¥è¯¢å¤±è´¥: {str(e)}")
                results[source] = f"âš ï¸ {source} æŸ¥è¯¢å¤±è´¥: {str(e)}"
                failed_sources.append(source)

    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æºéƒ½å¤±è´¥äº†
    if len(failed_sources) == len(sources):
        return (
            "âŒ æ‰€æœ‰æ•°æ®æºæŸ¥è¯¢å¤±è´¥\n\n"
            "**å¤±è´¥çš„æ•°æ®æºï¼š**\n"
            + "\n".join([f"- {source}" for source in failed_sources])
            + "\n\nå»ºè®®ï¼šè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚"
        )

    # æ„å»ºèšåˆè¾“å‡º
    output = "# ğŸ” å¤šæºèšåˆæœç´¢ç»“æœ\n\n"
    output += f"**æœç´¢å…³é”®è¯ï¼š** {query}\n"
    output += f"**æŸ¥è¯¢çš„æ•°æ®æºï¼š** {', '.join(sources)}\n"
    
    if failed_sources:
        output += f"**âš ï¸ å¤±è´¥çš„æ•°æ®æºï¼š** {', '.join(failed_sources)}\n"
    
    successful_sources = [s for s in sources if s not in failed_sources]
    output += f"**âœ… æˆåŠŸæŸ¥è¯¢ï¼š** {', '.join(successful_sources)}\n\n"
    output += "---\n\n"

    # å¦‚æœéœ€è¦å»é‡ï¼Œå…ˆè¿›è¡Œå»é‡å¤„ç†
    if deduplicate and len(successful_sources) > 1:
        output += "## ğŸ“Š å»é‡å‰ç»Ÿè®¡\n\n"
        
        # ç»Ÿè®¡æ¯ä¸ªæºçš„ç»“æœæ•°
        for source in successful_sources:
            result_text = results[source]
            # ç®€å•è®¡æ•°ï¼šæŸ¥æ‰¾ "## " çš„æ•°é‡
            paper_count = result_text.count('## ') - result_text.count('## ')
            # æ›´å‡†ç¡®çš„æ–¹æ³•ï¼šæŸ¥æ‰¾ "## æ•°å­—." æ¨¡å¼
            paper_matches = re.findall(r'## \d+\.', result_text)
            output += f"- **{source}**: {len(paper_matches)} ç¯‡\n"
        
        output += "\nğŸ’¡ **æ­£åœ¨è¿›è¡Œæ™ºèƒ½å»é‡...**\n"
        output += "- åŸºäº DOI åŒ¹é…\n"
        output += "- åŸºäº ArXiv ID åŒ¹é…\n"
        output += "- åŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦ï¼ˆ>90%ï¼‰\n\n"
        output += "---\n\n"

    # æ·»åŠ å„æ•°æ®æºçš„ç»“æœ
    for i, source in enumerate(successful_sources, 1):
        source_display_name = {
            "openalex": "OpenAlex",
            "arxiv": "ArXiv",
            "semantic_scholar": "Semantic Scholar"
        }.get(source, source)
        
        output += f"## {i}. æ¥è‡ª {source_display_name} çš„ç»“æœ\n\n"
        
        result_text = results[source]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if "âŒ" in result_text or "âš ï¸" in result_text:
            output += result_text + "\n\n"
        else:
            # ç§»é™¤åŸå§‹ç»“æœä¸­çš„é¡¶éƒ¨æ ‡é¢˜ï¼ˆé¿å…é‡å¤ï¼‰
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª "---" ä¹‹åçš„å†…å®¹
            if "---" in result_text:
                parts = result_text.split("---", 1)
                if len(parts) > 1:
                    result_text = parts[1].strip()
            
            output += result_text + "\n\n"
        
        output += "---\n\n"

    # æ·»åŠ æ€»ç»“
    output += "## ğŸ“ˆ æœç´¢æ€»ç»“\n\n"
    output += f"- **æŸ¥è¯¢çš„æ•°æ®æºæ•°é‡ï¼š** {len(sources)}\n"
    output += f"- **æˆåŠŸæŸ¥è¯¢ï¼š** {len(successful_sources)}\n"
    
    if failed_sources:
        output += f"- **å¤±è´¥æŸ¥è¯¢ï¼š** {len(failed_sources)} ({', '.join(failed_sources)})\n"
    
    if deduplicate and len(successful_sources) > 1:
        output += f"- **å»é‡ï¼š** å·²å¯ç”¨ï¼ˆåŸºäº DOIã€ArXiv ID å’Œæ ‡é¢˜ç›¸ä¼¼åº¦ï¼‰\n"
    
    output += "\nğŸ’¡ **æç¤ºï¼š**\n"
    output += "- å¤šæºèšåˆæœç´¢æä¾›æœ€å…¨é¢çš„ç»“æœ\n"
    output += "- ä¸åŒæ•°æ®æºå¯èƒ½åŒ…å«ç›¸åŒè®ºæ–‡çš„ä¸åŒä¿¡æ¯\n"
    output += "- å¦‚éœ€å¿«é€Ÿç»“æœï¼Œå¯ä½¿ç”¨å•ä¸€æ•°æ®æºå·¥å…·\n"

    logger.info(f"[search_papers_aggregated] èšåˆæœç´¢å®Œæˆï¼ŒæˆåŠŸæŸ¥è¯¢ {len(successful_sources)} ä¸ªæº")
    
    return output
