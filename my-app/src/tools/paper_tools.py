"""è®ºæ–‡æœç´¢å·¥å…·
åªåŒ…å« OpenAlex è®ºæ–‡æœç´¢åŠŸèƒ½
"""

from langchain.tools import tool
from pydantic import BaseModel, Field
from loguru import logger
import requests
from typing import Optional, Literal
import re
import os


def _translate_query_to_english(query: str) -> tuple[str, bool]:
    """å°†æŸ¥è¯¢ç¿»è¯‘ä¸ºè‹±æ–‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    
    Args:
        query: åŸå§‹æŸ¥è¯¢å­—ç¬¦ä¸²
        
    Returns:
        tuple: (ç¿»è¯‘åçš„æŸ¥è¯¢, æ˜¯å¦è¿›è¡Œäº†ç¿»è¯‘)
    """
    # æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
    has_chinese = bool(re.search(r'[\u4e00-\u9fff]', query))
    
    if not has_chinese:
        logger.info(f"[_translate_query] æŸ¥è¯¢å·²æ˜¯è‹±æ–‡ï¼Œæ— éœ€ç¿»è¯‘: {query}")
        return query, False
    
    logger.info(f"[_translate_query] æ£€æµ‹åˆ°ä¸­æ–‡æŸ¥è¯¢ï¼Œå‡†å¤‡ç¿»è¯‘: {query}")
    
    try:
        # åŠ¨æ€å¯¼å…¥æ¨¡å‹ï¼Œé¿å…å¾ªç¯ä¾èµ–
        from models import get_siliconflow_model, get_tongyi_model
        
        # æ ¹æ®ç¯å¢ƒå˜é‡é€‰æ‹©æ¨¡å‹
        llm_provider = os.getenv("LLM_PROVIDER", "tongyi").lower()
        
        if llm_provider == "siliconflow":
            model_name = os.getenv("SILICONFLOW_MODEL", "Qwen/Qwen2.5-7B-Instruct")
            model = get_siliconflow_model(
                model_name=model_name,
                streaming=False,
                temperature=0.1  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„ç¿»è¯‘
            )
        else:
            model = get_tongyi_model(
                streaming=False,
                temperature=0.1
            )
        
        # æ„å»ºç¿»è¯‘æç¤ºè¯
        translation_prompt = f"""è¯·å°†ä»¥ä¸‹ä¸­æ–‡å­¦æœ¯æœç´¢æŸ¥è¯¢ç¿»è¯‘ä¸ºå‡†ç¡®çš„è‹±æ–‡å­¦æœ¯æœ¯è¯­ã€‚
åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€‚ä¿æŒä¸“ä¸šå­¦æœ¯ç”¨è¯­ã€‚

ä¸­æ–‡æŸ¥è¯¢: {query}
è‹±æ–‡ç¿»è¯‘:"""
        
        # è°ƒç”¨æ¨¡å‹è¿›è¡Œç¿»è¯‘
        response = model.invoke(translation_prompt)
        translated_query = response.content.strip()
        
        # æ¸…ç†å¯èƒ½çš„å¼•å·å’Œå¤šä½™ç©ºæ ¼
        translated_query = translated_query.strip('"\'')
        translated_query = ' '.join(translated_query.split())
        
        logger.info(f"[_translate_query] ç¿»è¯‘æˆåŠŸ: '{query}' -> '{translated_query}'")
        return translated_query, True
        
    except Exception as e:
        logger.error(f"[_translate_query] ç¿»è¯‘å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
        logger.exception(e)
        # ç¿»è¯‘å¤±è´¥æ—¶è¿”å›åŸå§‹æŸ¥è¯¢
        return query, False


class OpenAlexSearchInput(BaseModel):
    """OpenAlex è®ºæ–‡æœç´¢çš„è¾“å…¥å‚æ•°"""

    query: str = Field(
        description="""æœç´¢å…³é”®è¯æˆ–çŸ­è¯­ï¼Œä¼šåœ¨è®ºæ–‡æ ‡é¢˜ã€æ‘˜è¦å’Œå…¨æ–‡ä¸­æœç´¢ã€‚
        
ç¤ºä¾‹ï¼š
- 'machine learning'ï¼ˆæœºå™¨å­¦ä¹ ï¼‰
- 'CRISPR gene editing'ï¼ˆåŸºå› ç¼–è¾‘ï¼‰
- 'climate change impact'ï¼ˆæ°”å€™å˜åŒ–å½±å“ï¼‰

æç¤ºï¼šä½¿ç”¨è‹±æ–‡å…³é”®è¯æ•ˆæœæœ€ä½³ï¼Œå¯ä»¥ä½¿ç”¨å¤šä¸ªè¯ç»„åˆ"""
    )

    max_results: int = Field(
        default=10,
        description="""è¿”å›çš„æœ€å¤§è®ºæ–‡æ•°é‡ã€‚
        
é»˜è®¤ï¼š10 ç¯‡
å»ºè®®èŒƒå›´ï¼š5-50 ç¯‡
æœ€å¤§ï¼š200 ç¯‡

ç”¨æˆ·è¯´æ³•æ˜ å°„ï¼š
- 'å‡ ç¯‡' / 'ä¸€äº›' â†’ 5-10
- 'å¾ˆå¤š' / 'å°½å¯èƒ½å¤š' â†’ 50-100
- æœªæåŠ â†’ ä½¿ç”¨é»˜è®¤ 10""",
        ge=1,
        le=200,
    )

    sort_by: Literal["relevance", "publication_date", "cited_by_count"] = Field(
        default="relevance",
        description="""ç»“æœæ’åºæ–¹å¼ã€‚
        
é€‰é¡¹è¯´æ˜ï¼š
- 'relevance'ï¼ˆç›¸å…³æ€§ï¼‰ï¼šæŒ‰ä¸æœç´¢è¯çš„åŒ¹é…åº¦æ’åºã€é»˜è®¤ã€‘
- 'publication_date'ï¼ˆå‘è¡¨æ—¥æœŸï¼‰ï¼šæŒ‰å‘è¡¨æ—¶é—´ä»æ–°åˆ°æ—§æ’åº
- 'cited_by_count'ï¼ˆå¼•ç”¨æ¬¡æ•°ï¼‰ï¼šæŒ‰è¢«å¼•ç”¨æ¬¡æ•°ä»é«˜åˆ°ä½æ’åº

ç”¨æˆ·è¯´æ³•æ˜ å°„ï¼š
- 'æœ€ç›¸å…³çš„' / 'åŒ¹é…åº¦é«˜çš„' â†’ relevance
- 'æœ€æ–°çš„' / 'æœ€è¿‘çš„' / 'æœ€è¿‘å‘è¡¨çš„' â†’ publication_date  
- 'æœ€æœ‰å½±å“åŠ›' / 'è¢«å¼•ç”¨æœ€å¤š' / 'é«˜å¼•ç”¨' â†’ cited_by_count
- æœªæåŠ â†’ ä½¿ç”¨é»˜è®¤ relevance""",
    )

    publication_year: Optional[str] = Field(
        default=None,
        description="""å‘è¡¨å¹´ä»½ç­›é€‰ï¼Œæ”¯æŒå¤šç§æ ¼å¼ã€‚
        
æ ¼å¼è¯´æ˜ï¼š
- '2023'ï¼šç²¾ç¡®åŒ¹é… 2023 å¹´
- '>2020'ï¼š2021 å¹´åŠä»¥åï¼ˆ2021, 2022, 2023...ï¼‰
- '<2020'ï¼š2019 å¹´åŠä¹‹å‰
- '2020-2023'ï¼š2020 åˆ° 2023 å¹´ä¹‹é—´ï¼ˆåŒ…å«è¾¹ç•Œï¼‰

ç”¨æˆ·è¯´æ³•æ˜ å°„ï¼š
- 'æœ€è¿‘' / 'è¿‘æœŸ' / 'æœ€è¿‘å‡ å¹´' â†’ '>2021' æˆ– '>2022'
- '2023å¹´' / 'å»å¹´' â†’ '2023'
- '2020å¹´åˆ°2023å¹´' / 'è¿‘ä¸‰å¹´' â†’ '2020-2023'
- '2020å¹´ä¹‹å' â†’ '>2020'
- æœªæåŠ â†’ Noneï¼ˆä¸é™åˆ¶å¹´ä»½ï¼‰

æ³¨æ„ï¼šå½“å‰å¹´ä»½æ˜¯ 2024 å¹´ï¼Œè¯·æ®æ­¤è®¡ç®—ç›¸å¯¹æ—¶é—´""",
    )

    open_access_only: bool = Field(
        default=False,
        description="""æ˜¯å¦ä»…è¿”å›å¼€æ”¾è·å–ï¼ˆOpen Access, OAï¼‰è®ºæ–‡ã€‚
        
- Trueï¼šåªè¿”å›å¯ä»¥å…è´¹ä¸‹è½½å…¨æ–‡çš„è®ºæ–‡
- Falseï¼šè¿”å›æ‰€æœ‰è®ºæ–‡ï¼ˆåŒ…æ‹¬éœ€è¦è®¢é˜…çš„ï¼‰ã€é»˜è®¤ã€‘

ç”¨æˆ·è¯´æ³•æ˜ å°„ï¼š
- 'å…è´¹' / 'å…è´¹ä¸‹è½½' / 'å¼€æ”¾è·å–' / 'OA' / 'èƒ½ä¸‹è½½çš„' â†’ True
- 'ä¸é™' / æœªæåŠ â†’ False""",
    )

    cited_by_count_min: Optional[int] = Field(
        default=None,
        description="""æœ€å°å¼•ç”¨æ¬¡æ•°ç­›é€‰ï¼Œåªè¿”å›è¢«å¼•ç”¨æ¬¡æ•°è¾¾åˆ°æ­¤å€¼çš„è®ºæ–‡ã€‚
        
ç”¨äºç­›é€‰é«˜å½±å“åŠ›çš„è®ºæ–‡ã€‚

ç”¨æˆ·è¯´æ³•æ˜ å°„ï¼š
- 'é«˜å¼•ç”¨' / 'æœ‰å½±å“åŠ›çš„' â†’ 50 æˆ– 100
- 'è¢«å¼•ç”¨å¾ˆå¤š' â†’ 100
- 'è‡³å°‘è¢«å¼•ç”¨Xæ¬¡' â†’ X
- 'å¼•ç”¨æ¬¡æ•°è¶…è¿‡X' â†’ X
- æœªæåŠ â†’ Noneï¼ˆä¸é™åˆ¶å¼•ç”¨æ¬¡æ•°ï¼‰

å‚è€ƒå€¼ï¼š
- 10+ï¼šæœ‰ä¸€å®šå½±å“åŠ›
- 50+ï¼šè¾ƒé«˜å½±å“åŠ›  
- 100+ï¼šé«˜å½±å“åŠ›
- 500+ï¼šéå¸¸é«˜å½±å“åŠ›""",
    )


@tool(args_schema=OpenAlexSearchInput)
def search_papers_openalex(
    query: str,
    max_results: int = 10,
    sort_by: Literal["relevance", "publication_date", "cited_by_count"] = "relevance",
    publication_year: Optional[str] = None,
    open_access_only: bool = False,
    cited_by_count_min: Optional[int] = None,
) -> str:
    """æœç´¢ OpenAlex å­¦æœ¯è®ºæ–‡æ•°æ®åº“ - 240M+ ç¯‡å­¦æœ¯æ–‡çŒ®ï¼Œè¦†ç›–æ‰€æœ‰å­¦ç§‘é¢†åŸŸã€‚

    OpenAlex æ˜¯å¼€æ”¾çš„å­¦æœ¯æ•°æ®åº“ï¼ŒåŒ…å«æœŸåˆŠæ–‡ç« ã€ä¼šè®®è®ºæ–‡ã€é¢„å°æœ¬ç­‰ã€‚æä¾›å¼•ç”¨æ•°æ®ã€
    å¼€æ”¾è·å–çŠ¶æ€ã€ä¸»é¢˜åˆ†ç±»ç­‰ä¸°å¯Œä¿¡æ¯ã€‚é€‚åˆå„å­¦ç§‘é¢†åŸŸçš„è®ºæ–‡æœç´¢å’Œå­¦æœ¯ç ”ç©¶ã€‚

    Args:
        query: æœç´¢å…³é”®è¯ï¼Œåœ¨æ ‡é¢˜ã€æ‘˜è¦ã€å…¨æ–‡ä¸­æœç´¢
        max_results: è¿”å›è®ºæ–‡æ•°é‡ï¼Œé»˜è®¤ 10 ç¯‡ï¼Œæœ€å¤§ 200 ç¯‡
        sort_by: æ’åºæ–¹å¼ï¼Œé»˜è®¤ç›¸å…³æ€§
        publication_year: å¹´ä»½ç­›é€‰ï¼Œæ”¯æŒå•å¹´ã€èŒƒå›´ã€æ¯”è¾ƒè¿ç®—
        open_access_only: æ˜¯å¦ä»…è¿”å›å¯å…è´¹è®¿é—®çš„å¼€æ”¾è·å–è®ºæ–‡
        cited_by_count_min: æœ€å°å¼•ç”¨æ¬¡æ•°ï¼Œç”¨äºç­›é€‰é«˜å½±å“åŠ›è®ºæ–‡
    """
    # ä¿å­˜åŸå§‹æŸ¥è¯¢ç”¨äºæ—¥å¿—
    original_query = query
    
    logger.info(
        f"[search_papers_openalex] æœç´¢è®ºæ–‡ - åŸå§‹æŸ¥è¯¢: {query}, æ•°é‡: {max_results}, æ’åº: {sort_by}"
    )
    
    # è‡ªåŠ¨ç¿»è¯‘ä¸­æ–‡æŸ¥è¯¢ä¸ºè‹±æ–‡
    translated_query, was_translated = _translate_query_to_english(query)
    query = translated_query  # ä½¿ç”¨ç¿»è¯‘åçš„æŸ¥è¯¢
    
    if was_translated:
        logger.info(f"[search_papers_openalex] æŸ¥è¯¢å·²ç¿»è¯‘: '{original_query}' -> '{query}'")
    
    # æ‰“å°æœç´¢å‚æ•°
    logger.info(f"[search_papers_openalex] æœç´¢å‚æ•°: query={query}, max_results={max_results}, sort_by={sort_by}")

    try:
        # æ˜ å°„æ’åºå‚æ•°åˆ° OpenAlex API æ ¼å¼
        sort_mapping = {
            "relevance": "relevance_score:desc",
            "publication_date": "publication_date:desc",
            "cited_by_count": "cited_by_count:desc",
        }
        api_sort = sort_mapping.get(sort_by, "relevance_score:desc")

        # æ„å»ºè¿‡æ»¤æ¡ä»¶åˆ—è¡¨ï¼ˆOpenAlex ä½¿ç”¨é€—å·åˆ†éš”çš„ filter å‚æ•°ï¼‰
        filters = []

        # å¹´ä»½ç­›é€‰
        if publication_year:
            filters.append(f"publication_year:{publication_year}")

        # å¼€æ”¾è·å–ç­›é€‰
        if open_access_only:
            filters.append("is_oa:true")

        # æœ€å°å¼•ç”¨æ¬¡æ•°ç­›é€‰
        if cited_by_count_min is not None:
            filters.append(f"cited_by_count:>{cited_by_count_min - 1}")

        # æ„å»º API è¯·æ±‚å‚æ•°
        base_url = "https://api.openalex.org/works"
        params = {
            "search": query,
            "per_page": max_results,  # OpenAlex æœ€ä½³å®è·µï¼šä½¿ç”¨æ›´å¤§çš„ per_page å€¼
            "sort": api_sort,
            "mailto": "347699233@qq.com",  # Polite pool: 10 req/sec (vs 1 req/sec)
        }

        # æ·»åŠ è¿‡æ»¤æ¡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
        if filters:
            params["filter"] = ",".join(filters)

        logger.debug(f"[search_papers_openalex] API å‚æ•°: {params}")

        # å‘é€ HTTP è¯·æ±‚ï¼ˆè¶…æ—¶ 30 ç§’ï¼ŒæŒ‰æ–‡æ¡£å»ºè®®ï¼‰
        response = requests.get(base_url, params=params, timeout=30)
        response.raise_for_status()

        # è§£æ JSON å“åº”
        data = response.json()
        results = data.get("results", [])
        meta = data.get("meta", {})
        total_count = meta.get("count", 0)

        # æ ¼å¼åŒ–è¾“å‡ºç»“æœ
        if not results:
            output = f"âŒ æœªæ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„è®ºæ–‡ã€‚\n"
            if was_translated:
                output += f"ğŸ’¡ åŸå§‹æŸ¥è¯¢: {original_query}\n"
                output += f"ğŸ”„ ç¿»è¯‘å: {query}\n"
            output += "å»ºè®®ï¼šå°è¯•æ›´é€šç”¨çš„å…³é”®è¯æˆ–æ£€æŸ¥æ‹¼å†™ã€‚"
            logger.warning("[search_papers_openalex] æœªæ‰¾åˆ°ç»“æœ")
        else:
            # æ„å»ºæœç´¢æ‘˜è¦
            filter_desc = []
            if publication_year:
                filter_desc.append(f"å¹´ä»½: {publication_year}")
            if open_access_only:
                filter_desc.append("ä»…å¼€æ”¾è·å–")
            if cited_by_count_min:
                filter_desc.append(f"å¼•ç”¨â‰¥{cited_by_count_min}")

            filter_text = f" ({', '.join(filter_desc)})" if filter_desc else ""
            output = f"ğŸ“š æ‰¾åˆ° {len(results)} ç¯‡è®ºæ–‡{filter_text}\n"
            
            # å¦‚æœè¿›è¡Œäº†ç¿»è¯‘ï¼Œæ˜¾ç¤ºç¿»è¯‘ä¿¡æ¯
            if was_translated:
                output += f"ğŸ’¡ åŸå§‹æŸ¥è¯¢: {original_query} â†’ è‹±æ–‡: {query}\n"
            
            output += f"ğŸ“Š æ•°æ®åº“æ€»è®¡: {total_count:,} ç¯‡ç›¸å…³æ–‡çŒ®\n\n"

            for idx, work in enumerate(results, 1):
                # æå–åŸºæœ¬ä¿¡æ¯
                title = work.get("title") or work.get("display_name", "æœªçŸ¥æ ‡é¢˜")
                doi = work.get("doi", "")
                pub_year = work.get("publication_year", "")
                pub_date = work.get("publication_date", "")
                cited_count = work.get("cited_by_count", 0)
                openalex_id = work.get("id", "")
                work_type = work.get("type", "").replace("-", " ").title()

                # æå–ä½œè€…ä¿¡æ¯ï¼ˆåªæ˜¾ç¤ºå‰ 3 ä½ï¼‰
                authorships = work.get("authorships", [])
                if authorships:
                    author_names = [
                        auth.get("author", {}).get("display_name", "æœªçŸ¥")
                        for auth in authorships[:3]
                    ]
                    if len(authorships) > 3:
                        authors = (
                            ", ".join(author_names) + f" ç­‰ ({len(authorships)} ä½ä½œè€…)"
                        )
                    else:
                        authors = ", ".join(author_names)
                else:
                    authors = "æœªçŸ¥ä½œè€…"

                # æå–æœŸåˆŠ/æ¥æºä¿¡æ¯
                primary_location = work.get("primary_location") or {}
                source = primary_location.get("source") or {}
                source_name = source.get("display_name", "")

                # æå–å¼€æ”¾è·å–ä¿¡æ¯
                open_access = work.get("open_access") or {}
                oa_url = open_access.get("oa_url", "")
                oa_status = open_access.get("oa_status", "closed")
                is_oa = open_access.get("is_oa", False)

                # æå–ç ”ç©¶ä¸»é¢˜ï¼ˆå‰ 2 ä¸ªï¼‰
                topics = work.get("topics") or []
                topic_names = [
                    t.get("display_name", "")
                    for t in topics[:2]
                    if t.get("display_name")
                ]

                # æ„å»ºè¾“å‡º - ä½¿ç”¨æ›´æ¸…æ™°çš„æ ¼å¼
                output += f"## {idx}. {title}\n\n"

                # åŸºæœ¬ä¿¡æ¯
                output += f"**ğŸ‘¥ ä½œè€…:** {authors}\n"
                if source_name:
                    output += f"**ğŸ“– æ¥æº:** {source_name}\n"
                if pub_year:
                    output += f"**ğŸ“… å‘è¡¨:** {pub_year}"
                    if pub_date and pub_date != pub_year:
                        output += f" ({pub_date})"
                    output += "\n"
                if work_type:
                    output += f"**ğŸ“„ ç±»å‹:** {work_type}\n"

                # å½±å“åŠ›æŒ‡æ ‡
                output += f"**ğŸ“Š å¼•ç”¨æ¬¡æ•°:** {cited_count:,}\n"

                # ç ”ç©¶ä¸»é¢˜
                if topic_names:
                    output += f"**ğŸ”¬ ç ”ç©¶ä¸»é¢˜:** {', '.join(topic_names)}\n"

                # é“¾æ¥å’Œè®¿é—®
                if doi:
                    # ç§»é™¤ DOI URL å‰ç¼€ï¼Œåªä¿ç•™ DOI
                    doi_clean = doi.replace("https://doi.org/", "")
                    output += f"**ğŸ”— DOI:** [{doi_clean}]({doi})\n"

                # å¼€æ”¾è·å–çŠ¶æ€
                if is_oa and oa_url:
                    oa_emoji = "ğŸ”“"
                    oa_text = {
                        "gold": "é‡‘è‰²å¼€æ”¾è·å–",
                        "green": "ç»¿è‰²å¼€æ”¾è·å–",
                        "hybrid": "æ··åˆå¼€æ”¾è·å–",
                        "bronze": "é“œè‰²å¼€æ”¾è·å–",
                    }.get(oa_status, "å¼€æ”¾è·å–")
                    output += f"**{oa_emoji} å…¨æ–‡è®¿é—®:** [{oa_text}]({oa_url})\n"
                else:
                    output += "**ğŸ”’ è®¿é—®:** éœ€è¦è®¢é˜…æˆ–ä»˜è´¹\n"

                # OpenAlex é“¾æ¥
                if openalex_id:
                    output += f"**ğŸ” è¯¦æƒ…:** {openalex_id}\n"

                output += "\n"

            logger.info(f"[search_papers_openalex] æˆåŠŸè¿”å› {len(results)} ç¯‡è®ºæ–‡")

        return output

    except requests.exceptions.Timeout:
        error_msg = "â±ï¸ OpenAlex API è¯·æ±‚è¶…æ—¶ï¼ˆ>30ç§’ï¼‰\nå»ºè®®ï¼šè¯·ç¨åé‡è¯•ï¼Œæˆ–å°è¯•æ›´å…·ä½“çš„æœç´¢æ¡ä»¶ã€‚"
        logger.error("[search_papers_openalex] è¯·æ±‚è¶…æ—¶")
        return error_msg

    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 403:
            error_msg = "ğŸš« å·²è¾¾åˆ° API é€Ÿç‡é™åˆ¶\nå»ºè®®ï¼šè¯·ç­‰å¾… 1 åˆ†é’Ÿåé‡è¯•ã€‚"
        elif e.response.status_code == 404:
            error_msg = "âŒ æœªæ‰¾åˆ°èµ„æº\nå»ºè®®ï¼šæ£€æŸ¥æœç´¢å‚æ•°æ˜¯å¦æ­£ç¡®ã€‚"
        elif e.response.status_code >= 500:
            error_msg = f"âš ï¸ OpenAlex æœåŠ¡å™¨é”™è¯¯ ({e.response.status_code})\nå»ºè®®ï¼šè¿™æ˜¯ä¸´æ—¶é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        else:
            error_msg = f"âŒ API è¯·æ±‚å¤±è´¥: HTTP {e.response.status_code}\n{str(e)}"
        logger.error(f"[search_papers_openalex] HTTPé”™è¯¯: {e.response.status_code}")
        return error_msg

    except requests.exceptions.RequestException as e:
        error_msg = f"ğŸŒ ç½‘ç»œè¿æ¥é”™è¯¯: {str(e)}\nå»ºè®®ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚"
        logger.error(f"[search_papers_openalex] ç½‘ç»œé”™è¯¯: {str(e)}")
        return error_msg

    except Exception as e:
        error_msg = f"âŒ æœç´¢è¿‡ç¨‹å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}\nå»ºè®®ï¼šè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚"
        logger.error("[search_papers_openalex] æœªçŸ¥é”™è¯¯")
        logger.exception(e)
        return error_msg
