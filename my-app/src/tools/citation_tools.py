"""å¼•ç”¨ç½‘ç»œåˆ†æå·¥å…·
ä½¿ç”¨ Semantic Scholar API åˆ†æè®ºæ–‡çš„å¼•ç”¨å…³ç³»
"""

import requests
import time
from typing import List, Dict, Optional
from langchain.tools import tool
from pydantic import BaseModel, Field
from loguru import logger


# ================== é…ç½® ==================

# Semantic Scholar API é…ç½®
S2_API_BASE = "https://api.semanticscholar.org/graph/v1"
S2_RATE_LIMIT = 1.0  # æ¯ç§’æœ€å¤š1ä¸ªè¯·æ±‚

# å­—æ®µé…ç½®ï¼šéœ€è¦è·å–çš„è®ºæ–‡å­—æ®µ
PAPER_FIELDS = [
    "paperId",
    "title",
    "abstract",
    "year",
    "authors",
    "citationCount",
    "influentialCitationCount",
    "venue",
    "publicationDate",
    "isOpenAccess",
    "openAccessPdf",
    "url",
]


# ================== é€Ÿç‡é™åˆ¶å™¨ ==================

class RateLimiter:
    """API è¯·æ±‚é€Ÿç‡é™åˆ¶å™¨"""
    
    def __init__(self, calls_per_second: float = 1.0):
        self.calls_per_second = calls_per_second
        self.last_call = 0.0
    
    def wait(self):
        """ç­‰å¾…ä»¥æ»¡è¶³é€Ÿç‡é™åˆ¶"""
        now = time.time()
        time_since_last_call = now - self.last_call
        if time_since_last_call < 1.0 / self.calls_per_second:
            sleep_time = 1.0 / self.calls_per_second - time_since_last_call
            time.sleep(sleep_time)
        self.last_call = time.time()


# å…¨å±€é€Ÿç‡é™åˆ¶å™¨
rate_limiter = RateLimiter(calls_per_second=S2_RATE_LIMIT)


# ================== è®ºæ–‡è¯†åˆ«å™¨ ==================

def identify_paper(paper_input: str) -> Optional[Dict]:
    """è¯†åˆ«è®ºæ–‡ï¼Œæ”¯æŒå¤šç§è¾“å…¥æ ¼å¼
    
    Args:
        paper_input: è®ºæ–‡æ ‡è¯†ï¼ˆæ ‡é¢˜/DOI/ArXiv ID/S2 ID/URLï¼‰
        
    Returns:
        è®ºæ–‡ä¿¡æ¯å­—å…¸ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å› None
    """
    logger.info(f"[identify_paper] è¯†åˆ«è®ºæ–‡: {paper_input[:100]}")
    
    # 1. å°è¯•ä½œä¸º Semantic Scholar ID
    if paper_input.startswith("CorpusId:") or len(paper_input) == 40:
        paper = get_paper_by_id(paper_input)
        if paper:
            logger.info("[identify_paper] âœ… é€šè¿‡ S2 ID æ‰¾åˆ°")
            return paper
    
    # 2. ä¼˜å…ˆæ£€æŸ¥ ArXiv IDï¼ˆåŒ…æ‹¬ DOI ä¸­çš„ ArXivï¼‰
    arxiv_id = extract_arxiv_id(paper_input)
    if arxiv_id:
        logger.debug(f"[identify_paper] å°è¯• ArXiv ID: {arxiv_id}")
        paper = search_paper_by_arxiv(arxiv_id)
        if paper:
            logger.info("[identify_paper] âœ… é€šè¿‡ ArXiv ID æ‰¾åˆ°")
            return paper
    
    # 3. å°è¯•ä½œä¸º DOIï¼ˆä½†æ’é™¤ arXiv DOIï¼Œå› ä¸ºå·²ç»åœ¨æ­¥éª¤2å¤„ç†ï¼‰
    if "10." in paper_input and "arxiv" not in paper_input.lower():
        doi = extract_doi(paper_input)
        if doi:
            logger.debug(f"[identify_paper] å°è¯• DOI: {doi}")
            paper = search_paper_by_doi(doi)
            if paper:
                logger.info("[identify_paper] âœ… é€šè¿‡ DOI æ‰¾åˆ°")
                return paper
    
    # 4. æœ€åå°è¯•æ ‡é¢˜æœç´¢ï¼ˆä½†è¦æ’é™¤æ˜æ˜¾çš„IDæ ¼å¼ï¼‰
    # å¦‚æœè¾“å…¥çœ‹èµ·æ¥åƒ ID è€Œä¸æ˜¯æ ‡é¢˜ï¼Œè·³è¿‡æ ‡é¢˜æœç´¢
    if not (paper_input.startswith("10.") or paper_input.startswith("http")):
        logger.debug("[identify_paper] å°è¯•æ ‡é¢˜æœç´¢")
        paper = search_paper_by_title(paper_input)
        if paper:
            logger.info("[identify_paper] âœ… é€šè¿‡æ ‡é¢˜æœç´¢æ‰¾åˆ°")
            return paper
    
    logger.warning(f"[identify_paper] âŒ æœªæ‰¾åˆ°è®ºæ–‡: {paper_input}")
    return None


def extract_arxiv_id(text: str) -> Optional[str]:
    """ä»æ–‡æœ¬ä¸­æå– ArXiv ID"""
    import re
    # åŒ¹é…æ ¼å¼ï¼š1234.5678 æˆ– arXiv:1234.5678
    patterns = [
        r'(?:arXiv:)?(\d{4}\.\d{4,5})',
        r'arxiv\.org/abs/(\d{4}\.\d{4,5})',
    ]
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return match.group(1)
    return None


def extract_doi(text: str) -> Optional[str]:
    """ä»æ–‡æœ¬ä¸­æå– DOI"""
    import re
    # åŒ¹é… DOI æ ¼å¼
    match = re.search(r'10\.\d{4,}/[^\s]+', text)
    if match:
        return match.group(0)
    return None


def get_paper_by_id(paper_id: str) -> Optional[Dict]:
    """é€šè¿‡ Semantic Scholar ID è·å–è®ºæ–‡"""
    rate_limiter.wait()
    url = f"{S2_API_BASE}/paper/{paper_id}"
    params = {"fields": ",".join(PAPER_FIELDS)}
    
    try:
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            return response.json()
        logger.warning(f"[get_paper_by_id] çŠ¶æ€ç  {response.status_code}")
    except Exception as e:
        logger.error(f"[get_paper_by_id] é”™è¯¯: {e}")
    
    return None


def search_paper_by_arxiv(arxiv_id: str) -> Optional[Dict]:
    """é€šè¿‡ ArXiv ID æœç´¢è®ºæ–‡"""
    rate_limiter.wait()
    url = f"{S2_API_BASE}/paper/arXiv:{arxiv_id}"
    params = {"fields": ",".join(PAPER_FIELDS)}
    
    try:
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            return response.json()
    except Exception as e:
        logger.error(f"[search_paper_by_arxiv] é”™è¯¯: {e}")
    
    return None


def search_paper_by_doi(doi: str) -> Optional[Dict]:
    """é€šè¿‡ DOI æœç´¢è®ºæ–‡"""
    rate_limiter.wait()
    url = f"{S2_API_BASE}/paper/DOI:{doi}"
    params = {"fields": ",".join(PAPER_FIELDS)}
    
    try:
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            return response.json()
    except Exception as e:
        logger.error(f"[search_paper_by_doi] é”™è¯¯: {e}")
    
    return None


def search_paper_by_title(title: str) -> Optional[Dict]:
    """é€šè¿‡æ ‡é¢˜æœç´¢è®ºæ–‡"""
    rate_limiter.wait()
    url = f"{S2_API_BASE}/paper/search"
    params = {
        "query": title,
        "fields": ",".join(PAPER_FIELDS),
        "limit": 1,
    }
    
    try:
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            data = response.json()
            if data.get("data") and len(data["data"]) > 0:
                return data["data"][0]
    except Exception as e:
        logger.error(f"[search_paper_by_title] é”™è¯¯: {e}")
    
    return None


# ================== å¼•ç”¨æ•°æ®è·å–å™¨ ==================

def get_references(paper_id: str, limit: int = 100) -> List[Dict]:
    """è·å–è®ºæ–‡çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨
    
    Args:
        paper_id: Semantic Scholar è®ºæ–‡ ID
        limit: æœ€å¤šè·å–çš„æ•°é‡
        
    Returns:
        å‚è€ƒæ–‡çŒ®åˆ—è¡¨
    """
    logger.info(f"[get_references] è·å–å‚è€ƒæ–‡çŒ®: {paper_id}, limit={limit}")
    rate_limiter.wait()
    
    url = f"{S2_API_BASE}/paper/{paper_id}/references"
    params = {
        "fields": ",".join(PAPER_FIELDS),
        "limit": limit,
    }
    
    try:
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            data = response.json()
            data_list = data.get("data") or []
            references = [item["citedPaper"] for item in data_list if item.get("citedPaper")]
            logger.info(f"[get_references] è·å–åˆ° {len(references)} ç¯‡å‚è€ƒæ–‡çŒ®")
            return references
        else:
            logger.warning(f"[get_references] API è¿”å›çŠ¶æ€ç : {response.status_code}")
    except Exception as e:
        logger.error(f"[get_references] é”™è¯¯: {e}")
    
    return []


def get_citations(paper_id: str, limit: int = 100) -> List[Dict]:
    """è·å–å¼•ç”¨è¯¥è®ºæ–‡çš„æ–‡çŒ®åˆ—è¡¨
    
    Args:
        paper_id: Semantic Scholar è®ºæ–‡ ID
        limit: æœ€å¤šè·å–çš„æ•°é‡
        
    Returns:
        å¼•ç”¨è®ºæ–‡åˆ—è¡¨
    """
    logger.info(f"[get_citations] è·å–å¼•ç”¨è®ºæ–‡: {paper_id}, limit={limit}")
    rate_limiter.wait()
    
    url = f"{S2_API_BASE}/paper/{paper_id}/citations"
    params = {
        "fields": ",".join(PAPER_FIELDS),
        "limit": limit,
    }
    
    try:
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            data = response.json()
            data_list = data.get("data") or []
            citations = [item["citingPaper"] for item in data_list if item.get("citingPaper")]
            logger.info(f"[get_citations] è·å–åˆ° {len(citations)} ç¯‡å¼•ç”¨è®ºæ–‡")
            return citations
        else:
            logger.warning(f"[get_citations] API è¿”å›çŠ¶æ€ç : {response.status_code}")
    except Exception as e:
        logger.error(f"[get_citations] é”™è¯¯: {e}")
    
    return []


# ================== é‡è¦æ€§è¯„åˆ†å™¨ ==================

def calculate_reference_score(paper: Dict) -> float:
    """è®¡ç®—å‚è€ƒæ–‡çŒ®çš„é‡è¦æ€§åˆ†æ•°
    
    è¯„åˆ†ç»´åº¦ï¼š
    - è¢«å¼•æ¬¡æ•°ï¼ˆ40åˆ†ï¼‰
    - å½±å“åŠ›å¼•ç”¨ï¼ˆ30åˆ†ï¼‰
    - å‘è¡¨å¹´ä»½ï¼ˆ15åˆ†ï¼Œæ—©æœŸç»å…¸åŠ åˆ†ï¼‰
    - å‘è¡¨æœŸåˆŠ/ä¼šè®®ï¼ˆ15åˆ†ï¼Œé¡¶ä¼šåŠ åˆ†ï¼‰
    
    Args:
        paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
        
    Returns:
        é‡è¦æ€§åˆ†æ•°ï¼ˆ0-100ï¼‰
    """
    score = 0.0
    
    # 1. è¢«å¼•æ¬¡æ•°ï¼ˆå½’ä¸€åŒ–åˆ°40åˆ†ï¼‰
    citation_count = paper.get("citationCount") or 0
    score += min(citation_count / 1000, 1.0) * 40
    
    # 2. å½±å“åŠ›å¼•ç”¨ï¼ˆå½’ä¸€åŒ–åˆ°30åˆ†ï¼‰
    influential_count = paper.get("influentialCitationCount") or 0
    score += min(influential_count / 100, 1.0) * 30
    
    # 3. å‘è¡¨å¹´ä»½ï¼ˆæ—©æœŸç»å…¸åŠ åˆ†ï¼‰
    year = paper.get("year")
    if year:
        if year < 2010:
            score += 15  # ç»å…¸è®ºæ–‡
        elif year < 2015:
            score += 10
        elif year < 2020:
            score += 5
    
    # 4. é¡¶ä¼šåŠ åˆ†
    venue = paper.get("venue", "").lower()
    top_venues = [
        "cvpr", "iccv", "eccv",  # è®¡ç®—æœºè§†è§‰
        "iclr", "neurips", "icml",  # æœºå™¨å­¦ä¹ 
        "acl", "emnlp", "naacl",  # NLP
        "aaai", "ijcai",  # AI
        "kdd", "www", "sigir",  # æ•°æ®æŒ–æ˜
    ]
    if any(v in venue for v in top_venues):
        score += 15
    
    return round(score, 1)


def calculate_citation_score(paper: Dict) -> float:
    """è®¡ç®—å¼•ç”¨è®ºæ–‡çš„é‡è¦æ€§åˆ†æ•°ï¼ˆSOTAè¯„åˆ†ï¼‰
    
    è¯„åˆ†ç»´åº¦ï¼š
    - å‘è¡¨å¹´ä»½ï¼ˆ40åˆ†ï¼Œè¿‘æœŸä¼˜å…ˆï¼‰
    - è¢«å¼•æ¬¡æ•°ï¼ˆ30åˆ†ï¼‰
    - å‘è¡¨æœŸåˆŠ/ä¼šè®®ï¼ˆ20åˆ†ï¼Œé¡¶ä¼šåŠ åˆ†ï¼‰
    - å¼€æ”¾è·å–ï¼ˆ10åˆ†ï¼‰
    
    Args:
        paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
        
    Returns:
        SOTA åˆ†æ•°ï¼ˆ0-100ï¼‰
    """
    score = 0.0
    
    # 1. å‘è¡¨å¹´ä»½ï¼ˆè¿‘æœŸä¼˜å…ˆï¼‰
    year = paper.get("year")
    if year:
        current_year = 2024  # å¯ä»¥æ”¹ä¸ºåŠ¨æ€è·å–
        if year >= current_year:
            score += 40
        elif year >= current_year - 1:
            score += 30
        elif year >= current_year - 2:
            score += 20
        elif year >= current_year - 3:
            score += 10
    
    # 2. è¢«å¼•æ¬¡æ•°ï¼ˆå½’ä¸€åŒ–åˆ°30åˆ†ï¼‰
    citation_count = paper.get("citationCount") or 0
    score += min(citation_count / 50, 1.0) * 30
    
    # 3. é¡¶ä¼šåŠ åˆ†
    venue = paper.get("venue", "").lower()
    top_venues = [
        "cvpr", "iccv", "eccv",
        "iclr", "neurips", "icml",
        "acl", "emnlp", "naacl",
        "aaai", "ijcai",
        "kdd", "www", "sigir",
    ]
    if any(v in venue for v in top_venues):
        score += 20
    
    # 4. å¼€æ”¾è·å–åŠ åˆ†
    if paper.get("isOpenAccess", False):
        score += 10
    
    return round(score, 1)


def rank_papers(papers: List[Dict], mode: str = "reference") -> List[Dict]:
    """å¯¹è®ºæ–‡åˆ—è¡¨è¿›è¡Œæ’åº
    
    Args:
        papers: è®ºæ–‡åˆ—è¡¨
        mode: æ’åºæ¨¡å¼ï¼Œ"reference"ï¼ˆå‚è€ƒæ–‡çŒ®ï¼‰æˆ– "citation"ï¼ˆå¼•ç”¨è®ºæ–‡ï¼‰
        
    Returns:
        æ’åºåçš„è®ºæ–‡åˆ—è¡¨ï¼ˆå¸¦æœ‰ importance_score å­—æ®µï¼‰
    """
    # è®¡ç®—åˆ†æ•°
    for paper in papers:
        if mode == "reference":
            paper["importance_score"] = calculate_reference_score(paper)
        else:
            paper["importance_score"] = calculate_citation_score(paper)
    
    # æŒ‰åˆ†æ•°é™åºæ’åº
    papers.sort(key=lambda x: x["importance_score"], reverse=True)
    
    return papers


# ================== æ ¼å¼åŒ–è¾“å‡º ==================

def format_paper_info(paper: Dict, index: int = None) -> str:
    """æ ¼å¼åŒ–å•ç¯‡è®ºæ–‡ä¿¡æ¯
    
    Args:
        paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
        index: åºå·ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
    """
    title = paper.get("title", "æ— æ ‡é¢˜")
    authors = paper.get("authors", [])
    author_names = [a.get("name", "") for a in authors[:3]]
    author_str = ", ".join(author_names)
    if len(authors) > 3:
        author_str += f" ç­‰ {len(authors)} ä½ä½œè€…"
    
    year = paper.get("year", "N/A")
    citation_count = paper.get("citationCount") or 0
    venue = paper.get("venue", "N/A")
    url = paper.get("url", "N/A")
    score = paper.get("importance_score") or 0
    
    # å¼€æ”¾è·å–æ ‡è®°
    oa_mark = "ğŸ”“" if paper.get("isOpenAccess", False) else ""
    
    prefix = f"### {index}. " if index else "### "
    
    return f"""{prefix}{title} {oa_mark}
- **ä½œè€…**: {author_str}
- **å¹´ä»½**: {year}
- **è¢«å¼•**: {citation_count} æ¬¡
- **æœŸåˆŠ/ä¼šè®®**: {venue}
- **é‡è¦æ€§è¯„åˆ†**: {score}/100
- **é“¾æ¥**: {url}

"""


def format_citation_network(
    center_paper: Dict,
    references: List[Dict],
    citations: List[Dict]
) -> str:
    """æ ¼å¼åŒ–å¼•ç”¨ç½‘ç»œåˆ†æç»“æœ
    
    Args:
        center_paper: ä¸­å¿ƒè®ºæ–‡
        references: å‚è€ƒæ–‡çŒ®åˆ—è¡¨
        citations: å¼•ç”¨è®ºæ–‡åˆ—è¡¨
        
    Returns:
        æ ¼å¼åŒ–çš„ Markdown å­—ç¬¦ä¸²
    """
    output = f"""# ğŸ“Š å¼•ç”¨ç½‘ç»œåˆ†æ

## ğŸ¯ ç›®æ ‡è®ºæ–‡

**{center_paper.get('title', 'æ— æ ‡é¢˜')}**

"""
    
    # è®ºæ–‡åŸºæœ¬ä¿¡æ¯
    authors = center_paper.get("authors", [])
    if authors:
        author_names = [a.get("name", "") for a in authors[:5]]
        output += f"- **ä½œè€…**: {', '.join(author_names)}"
        if len(authors) > 5:
            output += f" ç­‰ {len(authors)} ä½ä½œè€…"
        output += "\n"
    
    output += f"""- **å¹´ä»½**: {center_paper.get('year', 'N/A')}
- **è¢«å¼•æ¬¡æ•°**: {center_paper.get('citationCount', 0)} æ¬¡
- **å½±å“åŠ›å¼•ç”¨**: {center_paper.get('influentialCitationCount', 0)} æ¬¡
- **æœŸåˆŠ/ä¼šè®®**: {center_paper.get('venue', 'N/A')}
- **é“¾æ¥**: {center_paper.get('url', 'N/A')}

---

## ğŸ“š å‰ä¸–ï¼šé‡è¦å‚è€ƒæ–‡çŒ®ï¼ˆTop {len(references)}ï¼‰

è¿™äº›æ˜¯è¯¥è®ºæ–‡å¼•ç”¨çš„æœ€é‡è¦çš„å‚è€ƒæ–‡çŒ®ï¼Œä»£è¡¨äº†è¯¥ç ”ç©¶çš„ç†è®ºåŸºç¡€ï¼š

"""
    
    # å‚è€ƒæ–‡çŒ®åˆ—è¡¨
    for idx, ref in enumerate(references, 1):
        output += format_paper_info(ref, idx)
    
    output += f"""---

## ğŸš€ ä»Šç”Ÿï¼šå¼•ç”¨å®ƒçš„ SOTA è®ºæ–‡ï¼ˆTop {len(citations)}ï¼‰

è¿™äº›æ˜¯å¼•ç”¨äº†è¯¥è®ºæ–‡çš„æœ€æ–°é«˜è´¨é‡ç ”ç©¶ï¼Œä»£è¡¨äº†è¯¥æ–¹å‘çš„æœ€æ–°è¿›å±•ï¼š

"""
    
    # å¼•ç”¨è®ºæ–‡åˆ—è¡¨
    for idx, cite in enumerate(citations, 1):
        output += format_paper_info(cite, idx)
    
    # æ·»åŠ ç®€å•çš„ç»Ÿè®¡ä¿¡æ¯
    # è®¡ç®—å¹³å‡è¢«å¼•æ¬¡æ•°ï¼ˆé¿å…é™¤ä»¥0å’ŒNoneï¼‰
    avg_citations_ref = (
        sum(r.get('citationCount') or 0 for r in references) / len(references)
        if len(references) > 0 else 0
    )
    avg_citations_cite = (
        sum(c.get('citationCount') or 0 for c in citations) / len(citations)
        if len(citations) > 0 else 0
    )
    
    output += f"""---

## ğŸ“ˆ ç»Ÿè®¡æ‘˜è¦

- **å‚è€ƒæ–‡çŒ®æ€»æ•°**: æ£€æµ‹åˆ° {len(references)} ç¯‡é‡è¦å‚è€ƒæ–‡çŒ®
- **å¼•ç”¨è®ºæ–‡æ€»æ•°**: æ£€æµ‹åˆ° {len(citations)} ç¯‡å¼•ç”¨è®ºæ–‡
- **å¹³å‡è¢«å¼•æ¬¡æ•°ï¼ˆå‚è€ƒæ–‡çŒ®ï¼‰**: {avg_citations_ref:.1f} æ¬¡
- **å¹³å‡è¢«å¼•æ¬¡æ•°ï¼ˆå¼•ç”¨è®ºæ–‡ï¼‰**: {avg_citations_cite:.1f} æ¬¡
- **å¼€æ”¾è·å–è®ºæ–‡æ•°**: {sum(1 for p in references + citations if p.get('isOpenAccess', False))} ç¯‡

ğŸ’¡ **æç¤º**: æ ‡æœ‰ ğŸ”“ çš„è®ºæ–‡å¯ä»¥å…è´¹è·å–å…¨æ–‡ã€‚

"""
    
    # æ·»åŠ å¼•ç”¨å…³ç³»å¯è§†åŒ–å›¾ï¼ˆMermaidï¼‰
    if references or citations:
        output += generate_mermaid_graph(center_paper, references[:5], citations[:5])
    
    return output


def generate_mermaid_graph(
    center_paper: Dict,
    top_references: List[Dict],
    top_citations: List[Dict]
) -> str:
    """ç”Ÿæˆ Mermaid å¼•ç”¨å…³ç³»å›¾
    
    Args:
        center_paper: ä¸­å¿ƒè®ºæ–‡
        top_references: Top N å‚è€ƒæ–‡çŒ®
        top_citations: Top N å¼•ç”¨è®ºæ–‡
        
    Returns:
        Mermaid å›¾è¡¨çš„ Markdown ä»£ç 
    """
    center_title = center_paper.get('title', 'ç›®æ ‡è®ºæ–‡')[:40]
    center_year = center_paper.get('year', 'N/A')
    
    graph = """---

## ğŸ”— å¼•ç”¨å…³ç³»å¯è§†åŒ–

```mermaid
graph TB
    Center["ğŸ“„ {center_title}...<br/>({center_year})"]
    
    subgraph refs["ğŸ“š å‰ä¸– - é‡è¦å‚è€ƒæ–‡çŒ®"]
""".format(center_title=center_title, center_year=center_year)
    
    # æ·»åŠ å‚è€ƒæ–‡çŒ®èŠ‚ç‚¹
    for idx, ref in enumerate(top_references):
        ref_title = ref.get('title', 'å‚è€ƒæ–‡çŒ®')[:30]
        ref_year = ref.get('year', 'N/A')
        ref_citations = ref.get('citationCount') or 0
        
        # æ ¹æ®è¢«å¼•æ¬¡æ•°è®¾ç½®èŠ‚ç‚¹æ ·å¼
        if ref_citations > 10000:
            node_style = "Ref{idx}[/ğŸŒŸ {title}...<br/>({year}, {cites}å¼•)/]"
        elif ref_citations > 1000:
            node_style = "Ref{idx}[/{title}...<br/>({year}, {cites}å¼•)/]"
        else:
            node_style = "Ref{idx}[{title}...<br/>({year})]"
        
        graph += "        " + node_style.format(
            idx=idx,
            title=ref_title,
            year=ref_year,
            cites=ref_citations
        ) + "\n"
        graph += f"        Ref{idx} -->|å¼•ç”¨| Center\n"
    
    graph += "    end\n\n    subgraph cites[\"ğŸš€ ä»Šç”Ÿ - SOTA å¼•ç”¨è®ºæ–‡\"]\n"
    
    # æ·»åŠ å¼•ç”¨è®ºæ–‡èŠ‚ç‚¹
    for idx, cite in enumerate(top_citations):
        cite_title = cite.get('title', 'å¼•ç”¨è®ºæ–‡')[:30]
        cite_year = cite.get('year', 'N/A')
        cite_citations = cite.get('citationCount') or 0
        
        # æ ¹æ®å¹´ä»½è®¾ç½®èŠ‚ç‚¹æ ·å¼
        if cite_year and cite_year >= 2023:
            node_style = "Cite{idx}[\\ğŸ”¥ {title}...<br/>({year}, {cites}å¼•)\\]"
        elif cite_year and cite_year >= 2020:
            node_style = "Cite{idx}[\\{title}...<br/>({year}, {cites}å¼•)\\]"
        else:
            node_style = "Cite{idx}[{title}...<br/>({year})]"
        
        graph += "        " + node_style.format(
            idx=idx,
            title=cite_title,
            year=cite_year,
            cites=cite_citations
        ) + "\n"
        graph += f"        Center -->|è¢«å¼•ç”¨| Cite{idx}\n"
    
    graph += """    end
    
    style Center fill:#f9f,stroke:#333,stroke-width:4px
    style refs fill:#e1f5ff,stroke:#01579b,stroke-width:2px
    style cites fill:#fff3e0,stroke:#e65100,stroke-width:2px
```

**å›¾ä¾‹è¯´æ˜**ï¼š
- ğŸ“„ ä¸­å¿ƒèŠ‚ç‚¹ï¼šç›®æ ‡è®ºæ–‡
- ğŸ“š å·¦ä¾§ï¼šè¯¥è®ºæ–‡å¼•ç”¨çš„é‡è¦å‚è€ƒæ–‡çŒ®
- ğŸš€ å³ä¾§ï¼šå¼•ç”¨äº†è¯¥è®ºæ–‡çš„ SOTA ç ”ç©¶
- ğŸŒŸ é«˜è¢«å¼•è®ºæ–‡ï¼ˆ>10,000æ¬¡ï¼‰
- ğŸ”¥ æœ€æ–°è®ºæ–‡ï¼ˆ2023å¹´+ï¼‰

"""
    
    return graph


# ================== LangChain å·¥å…· ==================

class CitationNetworkInput(BaseModel):
    """å¼•ç”¨ç½‘ç»œåˆ†æå·¥å…·çš„è¾“å…¥å‚æ•°"""
    
    paper_identifier: str = Field(
        description="è®ºæ–‡æ ‡è¯†ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼šè®ºæ–‡æ ‡é¢˜ã€DOIã€ArXiv IDã€Semantic Scholar ID æˆ– URL"
    )
    max_references: int = Field(
        default=5,
        description="è¿”å›çš„æœ€å¤§å‚è€ƒæ–‡çŒ®æ•°é‡ï¼Œé»˜è®¤5ç¯‡"
    )
    max_citations: int = Field(
        default=5,
        description="è¿”å›çš„æœ€å¤§å¼•ç”¨è®ºæ–‡æ•°é‡ï¼Œé»˜è®¤5ç¯‡"
    )


@tool(args_schema=CitationNetworkInput)
def analyze_citation_network(
    paper_identifier: str,
    max_references: int = 5,
    max_citations: int = 5
) -> str:
    """åˆ†æè®ºæ–‡çš„å¼•ç”¨ç½‘ç»œï¼Œæ‰¾å‡ºé‡è¦çš„å‚è€ƒæ–‡çŒ®å’Œå¼•ç”¨å®ƒçš„SOTAè®ºæ–‡ã€‚
    
    è¿™ä¸ªå·¥å…·å¯ä»¥å¸®åŠ©ç”¨æˆ·å¿«é€Ÿäº†è§£ä¸€ç¯‡è®ºæ–‡çš„å­¦æœ¯å½±å“åŠ›å’Œç ”ç©¶è„‰ç»œï¼š
    - **å‰ä¸–**ï¼šè¿™ç¯‡è®ºæ–‡å¼•ç”¨çš„é‡è¦å‚è€ƒæ–‡çŒ®ï¼ˆç†è®ºåŸºç¡€ï¼‰
    - **ä»Šç”Ÿ**ï¼šå¼•ç”¨äº†è¿™ç¯‡è®ºæ–‡çš„æœ€æ–°SOTAç ”ç©¶ï¼ˆæœ€æ–°è¿›å±•ï¼‰
    
    ä½¿ç”¨åœºæ™¯ï¼š
    - å¿«é€Ÿäº†è§£æŸä¸ªç ”ç©¶æ–¹å‘çš„ç»å…¸è®ºæ–‡
    - è¿½è¸ªç ”ç©¶ä¸»é¢˜çš„æœ€æ–°è¿›å±•
    - å‘ç°ç›¸å…³çš„é«˜è´¨é‡è®ºæ–‡
    - æ„å»ºæ–‡çŒ®ç»¼è¿°
    
    Args:
        paper_identifier: è®ºæ–‡æ ‡è¯†ï¼ˆæ ‡é¢˜/DOI/ArXiv ID/URLï¼‰
        max_references: è¿”å›çš„æœ€å¤§å‚è€ƒæ–‡çŒ®æ•°é‡
        max_citations: è¿”å›çš„æœ€å¤§å¼•ç”¨è®ºæ–‡æ•°é‡
    
    Returns:
        æ ¼å¼åŒ–çš„å¼•ç”¨ç½‘ç»œåˆ†æç»“æœ
        
    ç¤ºä¾‹ï¼š
        analyze_citation_network("Attention is All You Need")
        analyze_citation_network("1706.03762")  # ArXiv ID
        analyze_citation_network("10.48550/arXiv.1706.03762")  # DOI
    """
    try:
        # 1. è¯†åˆ«è®ºæ–‡
        logger.info(f"[analyze_citation_network] å¼€å§‹åˆ†æ: {paper_identifier}")
        paper = identify_paper(paper_identifier)
        
        if not paper:
            return f"âŒ æœªæ‰¾åˆ°è®ºæ–‡: {paper_identifier}\n\nè¯·æ£€æŸ¥è¾“å…¥æ˜¯å¦æ­£ç¡®ï¼Œæ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š\n- è®ºæ–‡æ ‡é¢˜\n- ArXiv IDï¼ˆå¦‚ï¼š1706.03762ï¼‰\n- DOI\n- Semantic Scholar URL"
        
        paper_id = paper.get("paperId")
        paper_title = paper.get("title", "æœªçŸ¥")
        logger.info(f"[analyze_citation_network] æ‰¾åˆ°è®ºæ–‡: {paper_title}")
        
        # 2. å¹¶è¡Œè·å–å¼•ç”¨æ•°æ®
        references = get_references(paper_id, limit=100)
        citations = get_citations(paper_id, limit=100)
        
        if not references and not citations:
            return f"âš ï¸ æ‰¾åˆ°è®ºæ–‡ä½†æ— å¼•ç”¨æ•°æ®: {paper_title}\n\nè¿™å¯èƒ½æ˜¯ä¸€ç¯‡éå¸¸æ–°çš„è®ºæ–‡æˆ–æ•°æ®åº“ä¸­æš‚æ— å¼•ç”¨ä¿¡æ¯ã€‚"
        
        # 3. ç­›é€‰å’Œæ’åº
        if references:
            references = rank_papers(references, mode="reference")
            references = references[:max_references]
        
        if citations:
            citations = rank_papers(citations, mode="citation")
            citations = citations[:max_citations]
        
        # 4. æ ¼å¼åŒ–è¾“å‡º
        result = format_citation_network(paper, references, citations)
        
        logger.info(f"[analyze_citation_network] åˆ†æå®Œæˆ: {len(references)} ç¯‡å‚è€ƒæ–‡çŒ®, {len(citations)} ç¯‡å¼•ç”¨è®ºæ–‡")
        return result
        
    except Exception as e:
        error_msg = f"âŒ åˆ†æå¼•ç”¨ç½‘ç»œæ—¶å‡ºé”™: {str(e)}"
        logger.error(f"[analyze_citation_network] {error_msg}")
        return error_msg
